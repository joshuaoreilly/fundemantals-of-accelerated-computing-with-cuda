{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><div align=\"center\">Managing Accelerated Application Memory with CUDA C/C++ Unified Memory</div></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CUDA](./images/CUDA_Logo.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [*CUDA Best Practices Guide*](http://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#memory-optimizations), a highly recommended followup to this and other CUDA fundamentals labs, recommends a design cycle called **APOD**: **A**ssess, **P**arallelize, **O**ptimize, **D**eploy. In short, APOD prescribes an iterative design process, where developers can apply incremental improvements to their accelerated application's performance, and ship their code. As developers become more competent CUDA programmers, more advanced optimization techniques can be applied to their accelerated code bases.\n",
    "\n",
    "This lab will support such a style of iterative development. You will be using the Nsight Systems command line tool **nsys** to qualitatively measure your application's performance, and to identify opportunities for optimization, after which you will apply incremental improvements before learning new techniques and repeating the cycle. As a point of focus, many of the techniques you will be learning and applying in this lab will deal with the specifics of how CUDA's **Unified Memory** works. Understanding Unified Memory behavior is a fundamental skill for CUDA developers, and serves as a prerequisite to many more advanced memory management techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prerequisites\n",
    "\n",
    "To get the most out of this lab you should already be able to:\n",
    "\n",
    "- Write, compile, and run C/C++ programs that both call CPU functions and launch GPU kernels.\n",
    "- Control parallel thread hierarchy using execution configuration.\n",
    "- Refactor serial loops to execute their iterations in parallel on a GPU.\n",
    "- Allocate and free Unified Memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Objectives\n",
    "\n",
    "By the time you complete this lab, you will be able to:\n",
    "\n",
    "- Use the Nsight Systems command line tool (**nsys**) to profile accelerated application performance.\n",
    "- Leverage an understanding of **Streaming Multiprocessors** to optimize execution configurations.\n",
    "- Understand the behavior of **Unified Memory** with regard to page faulting and data migrations.\n",
    "- Use **asynchronous memory prefetching** to reduce page faults and data migrations for increased performance.\n",
    "- Employ an iterative development cycle to rapidly accelerate and deploy applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Iterative Optimizations with the NVIDIA Command Line Profiler\n",
    "\n",
    "The only way to be assured that attempts at optimizing accelerated code bases are actually successful is to profile the application for quantitative information about the application's performance. `nsys` is the Nsight Systems command line tool. It ships with the CUDA toolkit, and is a powerful tool for profiling accelerated applications.\n",
    "\n",
    "`nsys` is easy to use. Its most basic usage is to simply pass it the path to an executable compiled with `nvcc`. `nsys` will proceed to execute the application, after which it will print a summary output of the application's GPU activities, CUDA API calls, as well as information about **Unified Memory** activity, a topic which will be covered extensively later in this lab.\n",
    "\n",
    "When accelerating applications, or optimizing already-accelerated applications, take a scientific and iterative approach. Profile your application after making changes, take note, and record the implications of any refactoring on performance. Make these observations early and often: frequently, enough performance boost can be gained with little effort such that you can ship your accelerated application. Additionally, frequent profiling will teach you how specific changes to your CUDA code bases impact its actual performance: knowledge that is hard to acquire when only profiling after many kinds of changes in your code bases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Profile an Application with nsys\n",
    "\n",
    "[01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) (<------ you can click on this and any of the source file links in this lab to open them for editing) is a naively accelerated vector addition program. Use the two code execution cells below (`CTRL` + `ENTER`). The first code execution cell will compile (and run) the vector addition program. The second code execution cell will profile the executable that was just compiled using `nsys profile`.\n",
    "\n",
    "`nsys profile` will generate a `qdrep` report file which can be used in a variety of manners. We use the `--stats=true` flag here to indicate we would like summary statistics printed. There is quite a lot of information printed:\n",
    "\n",
    "- Profile configuration details\n",
    "- Report file(s) generation details\n",
    "- **CUDA API Statistics**\n",
    "- **CUDA Kernel Statistics**\n",
    "- **CUDA Memory Operation Statistics (time and size)**\n",
    "- OS Runtime API Statistics\n",
    "\n",
    "In this lab you will primarily be using the 3 sections in **bold** above. In the next lab, you will be using the generated report files to give to the Nsight Systems GUI for visual profiling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After profiling the application, answer the following questions using information displayed in the `CUDA Kernel Statistics` section of the profiling output:\n",
    "\n",
    "- What was the name of the only CUDA kernel called in this application? addVectorsInto\n",
    "- How many times did this kernel run? Once\n",
    "- How long did it take this kernel to run? Record this time somewhere: you will be optimizing this application and will want to know how much faster you can make it.\n",
    "\n",
    "Total time: 2284689086ns (2.28468909s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o single-thread-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-b93d-e79c-67c2-8f5f.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-b93d-e79c-67c2-8f5f.qdrep\"\n",
      "Exporting 4623 events: [==================================================100%][===========26%                                        ]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-b93d-e79c-67c2-8f5f.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls    Average      Minimum     Maximum            Name         \n",
      " -------  ---------------  ---------  ------------  ----------  ----------  ---------------------\n",
      "    88.0       2284697703          1  2284697703.0  2284697703  2284697703  cudaDeviceSynchronize\n",
      "    11.2        290305345          3    96768448.3       38439   290202301  cudaMallocManaged    \n",
      "     0.8         21054397          3     7018132.3     6318965     8185420  cudaFree             \n",
      "     0.0            68853          1       68853.0       68853       68853  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances    Average      Minimum     Maximum                       Name                    \n",
      " -------  ---------------  ---------  ------------  ----------  ----------  -------------------------------------------\n",
      "   100.0       2284689086          1  2284689086.0  2284689086  2284689086  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
      "    76.5         68390843        2304  29683.5     1855   175165  [CUDA Unified Memory memcpy HtoD]\n",
      "    23.5         20951444         768  27280.5     1150   162973  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average  Minimum  Maximum               Operation            \n",
      " ----------  ----------  -------  -------  --------  ---------------------------------\n",
      " 393216.000        2304  170.667    4.000  1020.000  [CUDA Unified Memory memcpy HtoD]\n",
      " 131072.000         768  170.667    4.000  1020.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum              Name           \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------------------\n",
      "    89.3       5339151156        273  19557330.2    37590  100130758  poll                      \n",
      "     8.6        514264646        242   2125060.5    15906   20644529  sem_timedwait             \n",
      "     1.7        100283816        678    147911.2     1065   17288218  ioctl                     \n",
      "     0.4         23514868         98    239947.6     1670    8112248  mmap                      \n",
      "     0.0          1844488         82     22493.8     4542      40916  open64                    \n",
      "     0.0           168100          4     42025.0    31015      46799  pthread_create            \n",
      "     0.0           166858          3     55619.3    53247      59909  fgets                     \n",
      "     0.0           154732         25      6189.3     1483      38147  fopen                     \n",
      "     0.0            88535         11      8048.6     4652      12495  write                     \n",
      "     0.0            62517         11      5683.4     3172      14100  munmap                    \n",
      "     0.0            35029          5      7005.8     3028       9745  open                      \n",
      "     0.0            31827         18      1768.2     1083       7778  fclose                    \n",
      "     0.0            29779          3      9926.3     3366      16971  fread                     \n",
      "     0.0            26408          5      5281.6     1063      11007  fgetc                     \n",
      "     0.0            23858         12      1988.2     1008       8864  fcntl                     \n",
      "     0.0            23270         13      1790.0     1021       3439  read                      \n",
      "     0.0            17294          2      8647.0     8143       9151  socket                    \n",
      "     0.0            13451          1     13451.0    13451      13451  sem_wait                  \n",
      "     0.0            11691          3      3897.0     1051       9466  pthread_rwlock_timedwrlock\n",
      "     0.0             8260          1      8260.0     8260       8260  pipe2                     \n",
      "     0.0             7909          4      1977.3     1672       2541  mprotect                  \n",
      "     0.0             7304          1      7304.0     7304       7304  connect                   \n",
      "     0.0             2872          1      2872.0     2872       2872  bind                      \n",
      "     0.0             1855          1      1855.0     1855       1855  listen                    \n",
      "\n",
      "Report file moved to \"/dli/task/report1.qdrep\"\n",
      "Report file moved to \"/dli/task/report1.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./single-thread-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worth mentioning is that by default, `nsys profile` will not overwrite an existing report file. This is done to prevent accidental loss of work when profiling. If for any reason, you would rather overwrite an existing report file, say during rapid iterations, you can provide the `-f` flag to `nsys profile` to allow overwriting an existing report file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Optimize and Profile\n",
    "\n",
    "Take a minute or two to make a simple optimization to [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) by updating its execution configuration so that it runs on many threads in a single thread block. Recompile and then profile with `nsys profile --stats=true` using the code execution cells below. Use the profiling output to check the runtime of the kernel. What was the speed up from this optimization? Be sure to record your results somewhere.\n",
    "\n",
    "With threadsPerBlock = 128 and numberOfBlocks = (N + threadsPerBlock - 1) / threadsPerBlock):\n",
    "\n",
    "    total time: 153630818ns = 0.15363082s (10x speedup)\n",
    "    \n",
    "with threadsPerBlock = 32:\n",
    "\n",
    "    total time: 150430974ns = 0.15043097 (10x speedup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o multi-thread-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-da44-3944-974d-dba7.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-da44-3944-974d-dba7.qdrep\"\n",
      "Exporting 4729 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-da44-3944-974d-dba7.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls    Average     Minimum    Maximum           Name         \n",
      " -------  ---------------  ---------  -----------  ---------  ---------  ---------------------\n",
      "    58.7        243830780          3   81276926.7      18343  243754977  cudaMallocManaged    \n",
      "    36.2        150441765          1  150441765.0  150441765  150441765  cudaDeviceSynchronize\n",
      "     5.1         20989387          3    6996462.3    6302934    8136145  cudaFree             \n",
      "     0.0            53401          1      53401.0      53401      53401  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances    Average     Minimum    Maximum                      Name                    \n",
      " -------  ---------------  ---------  -----------  ---------  ---------  -------------------------------------------\n",
      "   100.0        150430974          1  150430974.0  150430974  150430974  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
      "    76.7         69770511        2791  24998.4     2175   170716  [CUDA Unified Memory memcpy HtoD]\n",
      "    23.3         21177097         768  27574.3     1407   160029  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average  Minimum  Maximum               Operation            \n",
      " ----------  ----------  -------  -------  --------  ---------------------------------\n",
      " 393216.000        2791  140.887    4.000  1020.000  [CUDA Unified Memory memcpy HtoD]\n",
      " 131072.000         768  170.667    4.000  1020.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum              Name           \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------------------\n",
      "    84.4       1466124688         76  19291114.3    28084  100180523  poll                      \n",
      "     8.6        148604553         66   2251584.1    14491   20840029  sem_timedwait             \n",
      "     5.6         96936113        676    143396.6     1025   17431725  ioctl                     \n",
      "     1.3         23360263         98    238370.0     1337    8078706  mmap                      \n",
      "     0.1          1461324         82     17821.0     4487      29960  open64                    \n",
      "     0.0           166345          3     55448.3    52757      60203  fgets                     \n",
      "     0.0           147128          4     36782.0    32373      42066  pthread_create            \n",
      "     0.0           126535         25      5061.4     1457      21824  fopen                     \n",
      "     0.0            81520         11      7410.9     4316      11198  write                     \n",
      "     0.0            51297          4     12824.3     8114      14874  pthread_rwlock_timedwrlock\n",
      "     0.0            46328         11      4211.6     1862       6908  munmap                    \n",
      "     0.0            32639          5      6527.8     1656      13367  fgetc                     \n",
      "     0.0            30956          5      6191.2     3338       9156  open                      \n",
      "     0.0            27479         18      1526.6     1029       4809  fclose                    \n",
      "     0.0            23206         13      1785.1     1004       2965  read                      \n",
      "     0.0            22621          6      3770.2     1031      16950  fcntl                     \n",
      "     0.0            20168          2     10084.0     8632      11536  socket                    \n",
      "     0.0            10420          3      3473.3     1560       5049  fread                     \n",
      "     0.0             7930          4      1982.5     1724       2415  mprotect                  \n",
      "     0.0             7740          1      7740.0     7740       7740  connect                   \n",
      "     0.0             7640          1      7640.0     7640       7640  pipe2                     \n",
      "     0.0             2900          1      2900.0     2900       2900  bind                      \n",
      "     0.0             1604          1      1604.0     1604       1604  listen                    \n",
      "\n",
      "Report file moved to \"/dli/task/report3.qdrep\"\n",
      "Report file moved to \"/dli/task/report3.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./multi-thread-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Optimize Iteratively\n",
    "\n",
    "In this exercise you will go through several cycles of editing the execution configuration of [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu), profiling it, and recording the results to see the impact. Use the following guidelines while working:\n",
    "\n",
    "- Start by listing 3 to 5 different ways you will update the execution configuration, being sure to cover a range of different grid and block size combinations.\n",
    "- Edit the [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) program in one of the ways you listed.\n",
    "- Compile and profile your updated code with the two code execution cells below.\n",
    "- Record the runtime of the kernel execution, as given in the profiling output.\n",
    "- Repeat the edit/profile/record cycle for each possible optimization you listed above\n",
    "\n",
    "Which of the execution configurations you attempted proved to be the fastest?\n",
    "\n",
    "- Grid size: 4194304, Block size: 8, total time: 152898071ns\n",
    "- Grid size: 1048576, Block size: 32, total time: 151239831ns\n",
    "- Grid size: 262144, Block size: 128, total time: 141898366ns\n",
    "- Grid size: 65536, Block size: 512, total time: 147954972ns\n",
    "- Grid size: 32768, Block size: 1024, total time: as low as 125480675ns, as high as 145576306ns\n",
    "\n",
    "The largest block size runs the fastest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid size: 32768, Block size: 1024\n",
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o iteratively-optimized-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "Grid size: 32768, Block size: 1024\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-6de6-ef83-5f52-f16f.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-6de6-ef83-5f52-f16f.qdrep\"\n",
      "Exporting 10163 events: [=================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-6de6-ef83-5f52-f16f.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls    Average     Minimum    Maximum           Name         \n",
      " -------  ---------------  ---------  -----------  ---------  ---------  ---------------------\n",
      "    62.9        283487506          3   94495835.3      17821  283414634  cudaMallocManaged    \n",
      "    32.3        145587347          1  145587347.0  145587347  145587347  cudaDeviceSynchronize\n",
      "     4.7         21212950          3    7070983.3    6361623    8223294  cudaFree             \n",
      "     0.0            56871          1      56871.0      56871      56871  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances    Average     Minimum    Maximum                      Name                    \n",
      " -------  ---------------  ---------  -----------  ---------  ---------  -------------------------------------------\n",
      "   100.0        145576306          1  145576306.0  145576306  145576306  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
      "    78.8         78663444        8200   9593.1     2174   160637  [CUDA Unified Memory memcpy HtoD]\n",
      "    21.2         21212255         768  27620.1     1599   159868  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average  Minimum  Maximum               Operation            \n",
      " ----------  ----------  -------  -------  --------  ---------------------------------\n",
      " 393216.000        8200   47.953    4.000   960.000  [CUDA Unified Memory memcpy HtoD]\n",
      " 131072.000         768  170.667    4.000  1020.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum              Name           \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------------------\n",
      "    83.2       1443477967         74  19506459.0    53668  100128967  poll                      \n",
      "     8.4        145897241         64   2279644.4    16713   20525659  sem_timedwait             \n",
      "     6.9        118862091        680    174797.2     1046   18859319  ioctl                     \n",
      "     1.4         24035992         98    245265.2     1414    8165923  mmap                      \n",
      "     0.1          1850523         82     22567.4     6051      36080  open64                    \n",
      "     0.0           167019          3     55673.0    53162      60489  fgets                     \n",
      "     0.0           155507          4     38876.8    31753      45618  pthread_create            \n",
      "     0.0           131742         25      5269.7     1480      23708  fopen                     \n",
      "     0.0           131474         11     11952.2     5517      28579  write                     \n",
      "     0.0            50315         13      3870.4     2022       5858  munmap                    \n",
      "     0.0            46959         34      1381.1     1035       6006  fcntl                     \n",
      "     0.0            32430          5      6486.0     3170       9516  open                      \n",
      "     0.0            27598         18      1533.2     1026       4626  fclose                    \n",
      "     0.0            23275         12      1939.6     1092       3927  read                      \n",
      "     0.0            22351          4      5587.8     1636      11062  fgetc                     \n",
      "     0.0            14449          2      7224.5     5870       8579  socket                    \n",
      "     0.0            10108          1     10108.0    10108      10108  pipe2                     \n",
      "     0.0             9963          3      3321.0     1711       4348  fread                     \n",
      "     0.0             7866          4      1966.5     1873       2026  mprotect                  \n",
      "     0.0             7031          1      7031.0     7031       7031  connect                   \n",
      "     0.0             5668          1      5668.0     5668       5668  pthread_rwlock_timedwrlock\n",
      "     0.0             3070          1      3070.0     3070       3070  bind                      \n",
      "     0.0             1583          1      1583.0     1583       1583  listen                    \n",
      "\n",
      "Report file moved to \"/dli/task/report11.qdrep\"\n",
      "Report file moved to \"/dli/task/report11.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./iteratively-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Streaming Multiprocessors and Querying the Device\n",
    "\n",
    "This section explores how understanding a specific feature of the GPU hardware can promote optimization. After introducing **Streaming Multiprocessors**, you will attempt to further optimize the accelerated vector addition program you have been working on.\n",
    "\n",
    "The following slides present upcoming material visually, at a high level. Click through the slides before moving on to more detailed coverage of their topics in following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div align=\"center\"><iframe src=\"https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-01-V1/embedded/task2/NVPROF_UM_1.pptx\" width=\"800px\" height=\"500px\" frameborder=\"0\"></iframe></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "\n",
    "<div align=\"center\"><iframe src=\"https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-01-V1/embedded/task2/NVPROF_UM_1.pptx\" width=\"800px\" height=\"500px\" frameborder=\"0\"></iframe></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Multiprocessors and Warps\n",
    "\n",
    "The GPUs that CUDA applications run on have processing units called **streaming multiprocessors**, or **SMs**. During kernel execution, blocks of threads are given to SMs to execute. In order to support the GPU's ability to perform as many parallel operations as possible, performance gains can often be had by *choosing a grid size that has a number of blocks that is a multiple of the number of SMs on a given GPU.*\n",
    "\n",
    "Additionally, SMs create, manage, schedule, and execute groupings of 32 threads from within a block called **warps**. A more [in depth coverage of SMs and warps](http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation) is beyond the scope of this course, however, it is important to know that performance gains can also be had by *choosing a block size that has a number of threads that is a multiple of 32.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programmatically Querying GPU Device Properties\n",
    "\n",
    "In order to support portability, since the number of SMs on a GPU can differ depending on the specific GPU being used, the number of SMs should not be hard-coded into a code bases. Rather, this information should be acquired programatically.\n",
    "\n",
    "The following shows how, in CUDA C/C++, to obtain a C struct which contains many properties about the currently active GPU device, including its number of SMs:\n",
    "\n",
    "```cpp\n",
    "int deviceId;\n",
    "cudaGetDevice(&deviceId);                  // `deviceId` now points to the id of the currently active GPU.\n",
    "\n",
    "cudaDeviceProp props;\n",
    "cudaGetDeviceProperties(&props, deviceId); // `props` now has many useful properties about\n",
    "                                           // the active GPU device.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Query the Device\n",
    "\n",
    "Currently, [`01-get-device-properties.cu`](../edit/04-device-properties/01-get-device-properties.cu) contains many unassigned variables, and will print gibberish information intended to describe details about the currently active GPU.\n",
    "\n",
    "Build out [`01-get-device-properties.cu`](../edit/04-device-properties/01-get-device-properties.cu) to print the actual values for the desired device properties indicated in the source code. In order to support your work, and as an introduction to them, use the [CUDA Runtime Docs](http://docs.nvidia.com/cuda/cuda-runtime-api/structcudaDeviceProp.html) to help identify the relevant properties in the device props struct. Refer to [the solution](../edit/04-device-properties/solutions/01-get-device-properties-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID: 0\r\n",
      "Number of SMs: 40\r\n",
      "Compute Capability Major: 7\r\n",
      "Compute Capability Minor: 5\r\n",
      "Warp Size: 32\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o get-device-properties 04-device-properties/01-get-device-properties.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Optimize Vector Add with Grids Sized to Number of SMs\n",
    "\n",
    "Utilize your ability to query the device for its number of SMs to refactor the `addVectorsInto` kernel you have been working on inside [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) so that it launches with a grid containing a number of blocks that is a multiple of the number of SMs on the device.\n",
    "\n",
    "Depending on other specific details in the code you have written, this refactor may or may not improve, or significantly change, the performance of your kernel. Therefore, as always, be sure to use `nsys profile` so that you can quantitatively evaluate performance changes. Record the results with the rest of your findings thus far, based on the profiling output.\n",
    "\n",
    "- Grid size: 40, Block size: 1024, total time: 135458141ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid size: 40, Block size: 1024\n",
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o sm-optimized-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "Grid size: 40, Block size: 1024\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-f4d1-1461-fef4-4779.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-f4d1-1461-fef4-4779.qdrep\"\n",
      "Exporting 14140 events: [=================================================100%]7%                                                  ]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-f4d1-1461-fef4-4779.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls    Average     Minimum    Maximum           Name         \n",
      " -------  ---------------  ---------  -----------  ---------  ---------  ---------------------\n",
      "    62.8        264522004          3   88174001.3      17757  264437766  cudaMallocManaged    \n",
      "    32.2        135467044          1  135467044.0  135467044  135467044  cudaDeviceSynchronize\n",
      "     5.0         21119375          3    7039791.7    6368905    8166324  cudaFree             \n",
      "     0.0            59548          1      59548.0      59548      59548  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances    Average     Minimum    Maximum                      Name                    \n",
      " -------  ---------------  ---------  -----------  ---------  ---------  -------------------------------------------\n",
      "   100.0        135458141          1  135458141.0  135458141  135458141  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
      "    80.1         85249678       12192   6992.3     2174   166749  [CUDA Unified Memory memcpy HtoD]\n",
      "    19.9         21164943         768  27558.5     1471   160669  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average  Minimum  Maximum               Operation            \n",
      " ----------  ----------  -------  -------  --------  ---------------------------------\n",
      " 393216.000       12192   32.252    4.000  1000.000  [CUDA Unified Memory memcpy HtoD]\n",
      " 131072.000         768  170.667    4.000  1020.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum              Name           \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------------------\n",
      "    84.1       1462291279         76  19240674.7    32724  100129174  poll                      \n",
      "     8.6        149132069         66   2259576.8    22856   20612576  sem_timedwait             \n",
      "     5.8        101249165        686    147593.5     1062   17580985  ioctl                     \n",
      "     1.4         23583052         98    240643.4     1433    8104757  mmap                      \n",
      "     0.1          1487020         82     18134.4     4450      36409  open64                    \n",
      "     0.0           167259          3     55753.0    53318      60011  fgets                     \n",
      "     0.0           162748          4     40687.0    31632      48286  pthread_create            \n",
      "     0.0           121545         25      4861.8     1503      23108  fopen                     \n",
      "     0.0            84177         11      7652.5     4196      12853  write                     \n",
      "     0.0            44150         11      4013.6     1631       6798  munmap                    \n",
      "     0.0            32439          5      6487.8     3269       8845  open                      \n",
      "     0.0            28041         18      1557.8     1063       4606  fclose                    \n",
      "     0.0            27740         13      2133.8     1315       3621  read                      \n",
      "     0.0            21501          5      4300.2     1068      10029  fgetc                     \n",
      "     0.0            15305          2      7652.5     6288       9017  socket                    \n",
      "     0.0            14260          9      1584.4     1007       5430  fcntl                     \n",
      "     0.0            10277          1     10277.0    10277      10277  pthread_rwlock_timedwrlock\n",
      "     0.0             8931          3      2977.0     1554       3689  fread                     \n",
      "     0.0             8890          4      2222.5     1753       2723  mprotect                  \n",
      "     0.0             8752          1      8752.0     8752       8752  pipe2                     \n",
      "     0.0             7032          1      7032.0     7032       7032  connect                   \n",
      "     0.0             2540          1      2540.0     2540       2540  bind                      \n",
      "     0.0             1626          1      1626.0     1626       1626  listen                    \n",
      "\n",
      "Report file moved to \"/dli/task/report12.qdrep\"\n",
      "Report file moved to \"/dli/task/report12.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./sm-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Unified Memory Details\n",
    "\n",
    "You have been allocating memory intended for use either by host or device code with `cudaMallocManaged` and up until now have enjoyed the benefits of this method - automatic memory migration, ease of programming - without diving into the details of how the **Unified Memory** (**UM**) allocated by `cudaMallocManaged` actual works.\n",
    "\n",
    "`nsys profile` provides details about UM management in accelerated applications, and using this information, in conjunction with a more-detailed understanding of how UM works, provides additional opportunities to optimize accelerated applications.\n",
    "\n",
    "The following slides present upcoming material visually, at a high level. Click through the slides before moving on to more detailed coverage of their topics in following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div align=\"center\"><iframe src=\"https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-01-V1/embedded/task2/NVPROF_UM_2.pptx\" width=\"800px\" height=\"500px\" frameborder=\"0\"></iframe></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "\n",
    "<div align=\"center\"><iframe src=\"https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-01-V1/embedded/task2/NVPROF_UM_2.pptx\" width=\"800px\" height=\"500px\" frameborder=\"0\"></iframe></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unified Memory Migration\n",
    "\n",
    "When UM is allocated, the memory is not resident yet on either the host or the device. When either the host or device attempts to access the memory, a [page fault](https://en.wikipedia.org/wiki/Page_fault) will occur, at which point the host or device will migrate the needed data in batches. Similarly, at any point when the CPU, or any GPU in the accelerated system, attempts to access memory not yet resident on it, page faults will occur and trigger its migration.\n",
    "\n",
    "The ability to page fault and migrate memory on demand is tremendously helpful for ease of development in your accelerated applications. Additionally, when working with data that exhibits sparse access patterns, for example when it is impossible to know which data will be required to be worked on until the application actually runs, and for scenarios when data might be accessed by multiple GPU devices in an accelerated system with multiple GPUs, on-demand memory migration is remarkably beneficial.\n",
    "\n",
    "There are times - for example when data needs are known prior to runtime, and large contiguous blocks of memory are required - when the overhead of page faulting and migrating data on demand incurs an overhead cost that would be better avoided.\n",
    "\n",
    "Much of the remainder of this lab will be dedicated to understanding on-demand migration, and how to identify it in the profiler's output. With this knowledge you will be able to reduce the overhead of it in scenarios when it would be beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Explore UM Migration and Page Faulting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nsys profile` provides output describing UM behavior for the profiled application. In this exercise, you will make several modifications to a simple application, and make use of `nsys profile` after each change, to explore how UM data migration behaves.\n",
    "\n",
    "[`01-page-faults.cu`](../edit/06-unified-memory-page-faults/01-page-faults.cu) contains a `hostFunction` and a `gpuKernel`, both which could be used to initialize the elements of a `2<<24` element vector with the number `1`. Currently neither the host function nor GPU kernel are being used.\n",
    "\n",
    "For each of the 4 questions below, given what you have just learned about UM behavior, first hypothesize about what kind of page faulting should happen, then, edit [`01-page-faults.cu`](../edit/06-unified-memory-page-faults/01-page-faults.cu) to create a scenario, by using one or both of the 2 provided functions in the code bases, that will allow you to test your hypothesis.\n",
    "\n",
    "In order to test your hypotheses, compile and profile your code using the code execution cells below. Be sure to record your hypotheses, as well as the results, obtained from `nsys profile --stats=true` output. In the output of `nsys profile --stats=true` you should be looking for the following:\n",
    "\n",
    "- Is there a _CUDA Memory Operation Statistics_ section in the output?\n",
    "- If so, does it indicate host to device (HtoD) or device to host (DtoH) migrations?\n",
    "- When there are migrations, what does the output say about how many _Operations_ there were? If you see many small memory migration operations, this is a sign that on-demand page faulting is occurring, with small memory migrations occurring each time there is a page fault in the requested location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the scenarios for you to explore, along with solutions for them if you get stuck:\n",
    "\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed only by the CPU? ([solution](../edit/06-unified-memory-page-faults/solutions/01-page-faults-solution-cpu-only.cu))\n",
    "    - **Hypothesis**: no page faults/migration,\n",
    "    - **Result**: no CUDA Memory Operation Statistics section at all\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed only by the GPU? ([solution](../edit/06-unified-memory-page-faults/solutions/02-page-faults-solution-gpu-only.cu))\n",
    "    - **Hypothesis**: page fault and migration of vector in probably small chunks from host to device\n",
    "    - **Result**: also no CUDA Memory Operation Statistics section at all, I guess it makes a copy on both initially?\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed first by the CPU then the GPU? ([solution](../edit/06-unified-memory-page-faults/solutions/03-page-faults-solution-cpu-then-gpu.cu))\n",
    "    - **Hypothesis**: no migration/fault when running on host, then migration and fault when running on GPU, host to device\n",
    "    - **Result**: 768 host to device memcpy operations\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed first by the GPU then the CPU? ([solution](../edit/06-unified-memory-page-faults/solutions/04-page-faults-solution-gpu-then-cpu.cu))\n",
    "    - **Hypothesis**: no page faults/migration when first run on GPU, then 768 device to host memcpy operations\n",
    "    - **Results**: the hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o page-faults 06-unified-memory-page-faults/01-page-faults.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-7abf-cfec-37cb-1c40.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-7abf-cfec-37cb-1c40.qdrep\"\n",
      "Exporting 1977 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-7abf-cfec-37cb-1c40.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls    Average     Minimum    Maximum           Name         \n",
      " -------  ---------------  ---------  -----------  ---------  ---------  ---------------------\n",
      "    76.8        839136722          1  839136722.0  839136722  839136722  cudaDeviceSynchronize\n",
      "    22.4        244975265          1  244975265.0  244975265  244975265  cudaMallocManaged    \n",
      "     0.8          8493666          1    8493666.0    8493666    8493666  cudaFree             \n",
      "     0.0            65433          1      65433.0      65433      65433  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances    Average     Minimum    Maximum            Name          \n",
      " -------  ---------------  ---------  -----------  ---------  ---------  -----------------------\n",
      "   100.0        839127199          1  839127199.0  839127199  839127199  deviceKernel(int*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
      "   100.0         20955831         768  27286.2     1119   163389  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average  Minimum  Maximum               Operation            \n",
      " ----------  ----------  -------  -------  --------  ---------------------------------\n",
      " 131072.000         768  170.667    4.000  1020.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum              Name           \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------------------\n",
      "    86.7       2077123292        110  18882939.0    35578  100129661  poll                      \n",
      "     8.7        207291385         98   2115218.2    23435   20606544  sem_timedwait             \n",
      "     4.1         97431274        664    146733.8     1032   17467210  ioctl                     \n",
      "     0.5         10984777         92    119399.8     1504    8328972  mmap                      \n",
      "     0.1          1494213         82     18222.1     5244      31184  open64                    \n",
      "     0.0           167429          3     55809.7    53649      59707  fgets                     \n",
      "     0.0           147786          4     36946.5    33855      43231  pthread_create            \n",
      "     0.0           131946         25      5277.8     1500      20959  fopen                     \n",
      "     0.0            89314         11      8119.5     4221      16148  write                     \n",
      "     0.0            36395          6      6065.8     1164      15219  fgetc                     \n",
      "     0.0            31792          7      4541.7     2436       6648  munmap                    \n",
      "     0.0            31520          5      6304.0     3243       9073  open                      \n",
      "     0.0            28684         18      1593.6     1030       5080  fclose                    \n",
      "     0.0            27145          8      3393.1     1007      14365  fcntl                     \n",
      "     0.0            26845         13      2065.0     1417       3914  read                      \n",
      "     0.0            14543          2      7271.5     6398       8145  socket                    \n",
      "     0.0            11750          1     11750.0    11750      11750  pthread_rwlock_timedwrlock\n",
      "     0.0             8385          1      8385.0     8385       8385  pipe2                     \n",
      "     0.0             8068          2      4034.0     3532       4536  fread                     \n",
      "     0.0             7933          4      1983.3     1730       2418  mprotect                  \n",
      "     0.0             7660          1      7660.0     7660       7660  connect                   \n",
      "     0.0             2631          1      2631.0     2631       2631  bind                      \n",
      "     0.0             1585          1      1585.0     1585       1585  listen                    \n",
      "\n",
      "Report file moved to \"/dli/task/report17.qdrep\"\n",
      "Report file moved to \"/dli/task/report17.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./page-faults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Revisit UM Behavior for Vector Add Program\n",
    "\n",
    "Returning to the [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) program you have been working on throughout this lab, review the code bases in its current state, and hypothesize about what kinds of memory migrations and/or page faults you expect to occur. Look at the profiling output for your last refactor (either by scrolling up to find the output or by executing the code execution cell just below), observing the _CUDA Memory Operation Statistics_ section of the profiler output. Can you explain the kinds of migrations and the number of their operations based on the contents of the code base?\n",
    "\n",
    "Lots of host to device migrations, followed by a few device to host migrations.\n",
    "The vectors are allocated using `cudaMallocManaged()`, then initialized with `initWith()`, which is performed on the host.\n",
    "Since the kernel is called next, we need to send the three initialized vectors in their entirety to the device (thus the large number of memcpy HtoD)\n",
    "Finally, we have to verify the elements with `checkElementsAre()`, which requires passing back single result vector.\n",
    "\n",
    "If we look at the total size in KiB:\n",
    "\n",
    "- HtoD: `393216 KiB`\n",
    "- DtoH: `131072 KiB`\n",
    "\n",
    "Sure enough, we're passing three vectors worth of KiB to the device for the kernel, and one vector worth of KiB back to the device for the verification.\n",
    "If we look at the memory operation statistics by time, we can see that the HtoD operations take four times as long as the DtoH operations, despite only technically needing to move three times as many bytes.\n",
    "I suspect this is because the GPU is requesting a page of vector `a`, not getting it, having a page moved, then requesting a page of vector `b`, not getting it, requesting it, etc.\n",
    "Since we're adding the first element of each vector, then the second, we can really only grab a single value in each memcpy, and so have lots of page faults and need a large number of memcpy operations.\n",
    "In contrast, when copying the result vector back, we know that we're grabbing from the same contiguous block of device memory (since the result vector is presumably contiguous), so we can grab the largest pages possible on a page fault.\n",
    "Basically, cache coherence, but for unified memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "Grid size: 40, Block size: 1024\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-5a8e-a956-5183-0f66.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-5a8e-a956-5183-0f66.qdrep\"\n",
      "Exporting 16097 events: [=================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-5a8e-a956-5183-0f66.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls    Average     Minimum    Maximum           Name         \n",
      " -------  ---------------  ---------  -----------  ---------  ---------  ---------------------\n",
      "    59.4        243445698          3   81148566.0      18717  243365848  cudaMallocManaged    \n",
      "    35.4        145373893          1  145373893.0  145373893  145373893  cudaDeviceSynchronize\n",
      "     5.2         21262271          3    7087423.7    6404463    8196903  cudaFree             \n",
      "     0.0            59603          1      59603.0      59603      59603  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances    Average     Minimum    Maximum                      Name                    \n",
      " -------  ---------------  ---------  -----------  ---------  ---------  -------------------------------------------\n",
      "   100.0        145364335          1  145364335.0  145364335  145364335  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
      "    80.6         88268453       14149   6238.5     2143   168765  [CUDA Unified Memory memcpy HtoD]\n",
      "    19.4         21197936         768  27601.5     1567   160604  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average  Minimum  Maximum               Operation            \n",
      " ----------  ----------  -------  -------  --------  ---------------------------------\n",
      " 393216.000       14149   27.791    4.000  1012.000  [CUDA Unified Memory memcpy HtoD]\n",
      " 131072.000         768  170.667    4.000  1020.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum        Name     \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------\n",
      "    84.4       1470233530         77  19093941.9    31600  105107673  poll          \n",
      "     8.6        149468428         67   2230872.1    23114   20536880  sem_timedwait \n",
      "     5.5         95889019        685    139984.0     1090   17297205  ioctl         \n",
      "     1.4         23622211         98    241043.0     1442    8134892  mmap          \n",
      "     0.1          1782186         82     21734.0     4447      36389  open64        \n",
      "     0.0           167734          3     55911.3    53039      60811  fgets         \n",
      "     0.0           144122          4     36030.5    29123      41214  pthread_create\n",
      "     0.0           135867         25      5434.7     1525      34223  fopen         \n",
      "     0.0            76631         11      6966.5     4089      12645  write         \n",
      "     0.0            45860         11      4169.1     1898       6671  munmap        \n",
      "     0.0            29810          5      5962.0     3101       8639  open          \n",
      "     0.0            27428         13      2109.8     1375       3380  read          \n",
      "     0.0            26870         18      1492.8     1057       4186  fclose        \n",
      "     0.0            22705          6      3784.2     1080      10253  fgetc         \n",
      "     0.0            14895          8      1861.9     1001       6622  fcntl         \n",
      "     0.0            13833          2      6916.5     6343       7490  socket        \n",
      "     0.0            10091          3      3363.7     1647       4494  fread         \n",
      "     0.0             8073          4      2018.3     1812       2128  mprotect      \n",
      "     0.0             7201          1      7201.0     7201       7201  pipe2         \n",
      "     0.0             6943          1      6943.0     6943       6943  connect       \n",
      "     0.0             2243          1      2243.0     2243       2243  bind          \n",
      "     0.0             1636          1      1636.0     1636       1636  listen        \n",
      "\n",
      "Report file moved to \"/dli/task/report18.qdrep\"\n",
      "Report file moved to \"/dli/task/report18.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./sm-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Initialize Vector in Kernel\n",
    "\n",
    "When `nsys profile` gives the amount of time that a kernel takes to execute, the host-to-device page faults and data migrations that occur during this kernel's execution are included in the displayed execution time.\n",
    "\n",
    "With this in mind, refactor the `initWith` host function in your [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) program to instead be a CUDA kernel, initializing the allocated vector in parallel on the GPU. After successfully compiling and running the refactored application, but before profiling it, hypothesize about the following:\n",
    "\n",
    "- How do you expect the refactor to affect UM memory migration behavior?\n",
    "    - **Hypothesis**: with the first operation performed on each vector done on the device, we won't have any HtoD memcpy ops. There will still be the DtoH ops when verifying the result, as that is not yet a kernel.\n",
    "    - **Results**: hypothesis confirmed, no HtoD operations.\n",
    "- How do you expect the refactor to affect the reported run time of `addVectorsInto`?\n",
    "    - **Hypothesis**: runtime will be reduced by about the total time of all the HtoD memcpy ops, or about 88268453ns, bringing the total runtime to 57095882ns, minus overhead caused by additional operations in the kernel required to get the stride and index (and runtime of `addVectorsInto` will be reduced by the time the memcpy would usually take)\n",
    "    - **Results**: Total time reduced to about 57939902ns, which is about what I guess, minus overhead. `addVectorsInto`'s execution time shrank drastically, again by approximately the time required to move memory to the device.\n",
    "\n",
    "Once again, record the results. Refer to [the solution](../edit/07-init-in-kernel/solutions/01-vector-add-init-in-kernel-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid size: 40, Block size: 1024\n",
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o initialize-in-kernel 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "Grid size: 40, Block size: 1024\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-e732-809c-9869-13a6.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-e732-809c-9869-13a6.qdrep\"\n",
      "Exporting 1885 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-e732-809c-9869-13a6.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum           Name         \n",
      " -------  ---------------  ---------  ----------  -------  ---------  ---------------------\n",
      "    76.8        260194106          3  86731368.7    24586  260102732  cudaMallocManaged    \n",
      "    17.1         57931762          2  28965881.0  1886605   56045157  cudaDeviceSynchronize\n",
      "     6.1         20550288          3   6850096.0  6065628    8262513  cudaFree             \n",
      "     0.0            94676          4     23669.0     7524      44261  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances   Average    Minimum   Maximum                      Name                    \n",
      " -------  ---------------  ---------  ----------  --------  --------  -------------------------------------------\n",
      "    96.7         56053441          3  18684480.3  17514208  19915157  initWith(float, float*, int)               \n",
      "     3.3          1886461          1   1886461.0   1886461   1886461  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
      "   100.0         21244180         768  27661.7     1631   169437  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average  Minimum  Maximum               Operation            \n",
      " ----------  ----------  -------  -------  --------  ---------------------------------\n",
      " 131072.000         768  170.667    4.000  1020.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum              Name           \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------------------\n",
      "    77.0        709669186         36  19713032.9    28634  100201515  poll                      \n",
      "    10.9        100834147        687    146774.6     1027   17336176  ioctl                     \n",
      "     9.3         85444208         30   2848140.3    15417   21005672  sem_timedwait             \n",
      "     2.5         22938540         98    234066.7     1504    8207693  mmap                      \n",
      "     0.2          1440808         82     17570.8     4319      28793  open64                    \n",
      "     0.0           180654          4     45163.5    33799      56747  pthread_create            \n",
      "     0.0           167533          3     55844.3    53614      60099  fgets                     \n",
      "     0.0           142585         25      5703.4     1537      25813  fopen                     \n",
      "     0.0            82356         11      7486.9     4318      12724  write                     \n",
      "     0.0            49841         12      4153.4     2310       9946  munmap                    \n",
      "     0.0            44400          6      7400.0     1639      20425  fgetc                     \n",
      "     0.0            35171          5      7034.2     3250       9774  open                      \n",
      "     0.0            29749         18      1652.7     1058       5982  fclose                    \n",
      "     0.0            21194         13      1630.3     1080       2808  read                      \n",
      "     0.0            19422          2      9711.0     8419      11003  socket                    \n",
      "     0.0            17197          1     17197.0    17197      17197  sem_wait                  \n",
      "     0.0            15341          9      1704.6     1041       6047  fcntl                     \n",
      "     0.0            14536          2      7268.0     1083      13453  pthread_rwlock_timedwrlock\n",
      "     0.0            11604          3      3868.0     2714       5007  fread                     \n",
      "     0.0             9613          1      9613.0     9613       9613  connect                   \n",
      "     0.0             9157          4      2289.3     1923       2927  mprotect                  \n",
      "     0.0             7892          1      7892.0     7892       7892  pipe2                     \n",
      "     0.0             2929          1      2929.0     2929       2929  bind                      \n",
      "     0.0             1881          1      1881.0     1881       1881  listen                    \n",
      "\n",
      "Report file moved to \"/dli/task/report19.qdrep\"\n",
      "Report file moved to \"/dli/task/report19.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./initialize-in-kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Asynchronous Memory Prefetching\n",
    "\n",
    "A powerful technique to reduce the overhead of page faulting and on-demand memory migrations, both in host-to-device and device-to-host memory transfers, is called **asynchronous memory prefetching**. Using this technique allows programmers to asynchronously migrate unified memory (UM) to any CPU or GPU device in the system, in the background, prior to its use by application code. By doing this, GPU kernels and CPU function performance can be increased on account of reduced page fault and on-demand data migration overhead.\n",
    "\n",
    "Prefetching also tends to migrate data in larger chunks, and therefore fewer trips, than on-demand migration. This makes it an excellent fit when data access needs are known before runtime, and when data access patterns are not sparse.\n",
    "\n",
    "CUDA Makes asynchronously prefetching managed memory to either a GPU device or the CPU easy with its `cudaMemPrefetchAsync` function. Here is an example of using it to both prefetch data to the currently active GPU device, and then, to the CPU:\n",
    "\n",
    "```cpp\n",
    "int deviceId;\n",
    "cudaGetDevice(&deviceId);                                         // The ID of the currently active GPU device.\n",
    "\n",
    "cudaMemPrefetchAsync(pointerToSomeUMData, size, deviceId);        // Prefetch to GPU device.\n",
    "cudaMemPrefetchAsync(pointerToSomeUMData, size, cudaCpuDeviceId); // Prefetch to host. `cudaCpuDeviceId` is a\n",
    "                                                                  // built-in CUDA variable.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Prefetch Memory\n",
    "\n",
    "At this point in the lab, your [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) program should not only be launching a CUDA kernel to add 2 vectors into a third solution vector, all which are allocated with `cudaMallocManaged`, but should also be initializing each of the 3 vectors in parallel in a CUDA kernel. If for some reason, your application does not do any of the above, please refer to the following [reference application](../edit/07-init-in-kernel/solutions/01-vector-add-init-in-kernel-solution.cu), and update your own code bases to reflect its current functionality.\n",
    "\n",
    "Conduct 3 experiments using `cudaMemPrefetchAsync` inside of your [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) application to understand its impact on page-faulting and memory migration.\n",
    "\n",
    "**Overall Hypothesis**: since prefetching memory will move over entire vectors all at once, and not just when individual pages are requested, the page sizes can be much larger, speeding things up.\n",
    "When the kernel is actually running, there will be no page faults, since they're already on the device.\n",
    "\n",
    "- What happens when you prefetch one of the initialized vectors to the device?\n",
    "    - **Hypothesis**: `initWith()` will be a little faster, since one of the vectors is being fetched async.\n",
    "    - **Results**: `initWith()` runtime went from 56053441ns to 29334939ns\n",
    "- What happens when you prefetch two of the initialized vectors to the device?\n",
    "    - **Hypothesis**: `initWith()` will be even faster.\n",
    "    - **Results**: `initWith()` runtime went to 14956823ns\n",
    "- What happens when you prefetch all three of the initialized vectors to the device?\n",
    "    - **Hypothesis**: `initWith()` go zoom.\n",
    "    - **Results**: `initWith()` runtime is now 2056023ns, down from 56053441ns **(27x faster!)**\n",
    "\n",
    "Hypothesize about UM behavior, page faulting specifically, as well as the impact on the reported run time of the initialization kernel, before each experiment, and then verify by running `nsys profile`. Refer to [the solution](../edit/08-prefetch/solutions/01-vector-add-prefetch-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid size: 40, Block size: 1024\n",
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o prefetch-to-gpu 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "Grid size: 40, Block size: 1024\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-785f-270c-8ec9-7dd1.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-785f-270c-8ec9-7dd1.qdrep\"\n",
      "Exporting 1908 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-785f-270c-8ec9-7dd1.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum           Name         \n",
      " -------  ---------------  ---------  ----------  -------  ---------  ---------------------\n",
      "    90.6        240912050          3  80304016.7    17531  240850147  cudaMallocManaged    \n",
      "     6.9         18399266          3   6133088.7  1411400   15405889  cudaFree             \n",
      "     1.9          5179613          2   2589806.5  1891105    3288508  cudaDeviceSynchronize\n",
      "     0.6          1487355          3    495785.0    98331     720508  cudaMemPrefetchAsync \n",
      "     0.0            68256          4     17064.0     9689      34656  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances   Average   Minimum  Maximum                     Name                    \n",
      " -------  ---------------  ---------  ---------  -------  -------  -------------------------------------------\n",
      "    52.1          2056023          3   685341.0   680914   693970  initWith(float, float*, int)               \n",
      "    47.9          1889851          1  1889851.0  1889851  1889851  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
      "   100.0         21223011         768  27634.1     1598   172892  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average  Minimum  Maximum               Operation            \n",
      " ----------  ----------  -------  -------  --------  ---------------------------------\n",
      " 131072.000         768  170.667    4.000  1020.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum              Name           \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------------------\n",
      "    75.8        592780488         35  16936585.4    47577  100126569  poll                      \n",
      "    12.3         96457878        692    139390.0     1001   17277510  ioctl                     \n",
      "     8.9         69228112         29   2387176.3    15800   20778961  sem_timedwait             \n",
      "     2.6         20665978         99    208747.3     1329   15325642  mmap                      \n",
      "     0.2          1613648         82     19678.6     4535      36292  open64                    \n",
      "     0.0           186391          5     37278.2    31992      43223  pthread_create            \n",
      "     0.0           167903          3     55967.7    53356      61167  fgets                     \n",
      "     0.0           119816         25      4792.6     1422      23097  fopen                     \n",
      "     0.0           112470          2     56235.0    37373      75097  sem_wait                  \n",
      "     0.0           103891         12      8657.6     4149      13629  write                     \n",
      "     0.0            47260         23      2054.8     1004      14535  fcntl                     \n",
      "     0.0            45983         12      3831.9     1317       7446  munmap                    \n",
      "     0.0            30630          5      6126.0     3260       9527  open                      \n",
      "     0.0            27072         18      1504.0     1029       4560  fclose                    \n",
      "     0.0            22563         13      1735.6     1054       2907  read                      \n",
      "     0.0            20809          5      4161.8     1109       9620  fgetc                     \n",
      "     0.0            12369          2      6184.5     4884       7485  socket                    \n",
      "     0.0            11112          5      2222.4     1761       3002  mprotect                  \n",
      "     0.0             9259          3      3086.3     1887       3947  fread                     \n",
      "     0.0             9119          1      9119.0     9119       9119  pthread_rwlock_timedwrlock\n",
      "     0.0             7412          1      7412.0     7412       7412  pipe2                     \n",
      "     0.0             6240          1      6240.0     6240       6240  connect                   \n",
      "     0.0             2114          1      2114.0     2114       2114  bind                      \n",
      "     0.0             1576          1      1576.0     1576       1576  listen                    \n",
      "\n",
      "Report file moved to \"/dli/task/report22.qdrep\"\n",
      "Report file moved to \"/dli/task/report22.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./prefetch-to-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Prefetch Memory Back to the CPU\n",
    "\n",
    "Add additional prefetching back to the CPU for the function that verifies the correctness of the `addVectorInto` kernel. Again, hypothesize about the impact on UM before profiling in `nsys` to confirm. Refer to [the solution](../edit/08-prefetch/solutions/02-vector-add-prefetch-solution-cpu-also.cu) if you get stuck.\n",
    "\n",
    "- **Hypothesis**: since we're working with a single vector, page faults will grab large amounts of memory at once anyways, so prefetching shouldn't have much of a benefit (vs when alternating between three vectors).\n",
    "- **Results**: number of memcpy has gone down, average copy size has gone up, but copy time is virtually unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid size: 40, Block size: 1024\n",
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o prefetch-to-cpu 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "Grid size: 40, Block size: 1024\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-efe5-0a12-1688-7451.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-efe5-0a12-1688-7451.qdrep\"\n",
      "Exporting 1186 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-efe5-0a12-1688-7451.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum           Name         \n",
      " -------  ---------------  ---------  ----------  -------  ---------  ---------------------\n",
      "    81.8        255796759          3  85265586.3    18869  255732279  cudaMallocManaged    \n",
      "    13.2         41311200          4  10327800.0    86161   39897607  cudaMemPrefetchAsync \n",
      "     3.4         10498837          3   3499612.3   803515    8622524  cudaFree             \n",
      "     1.6          5137447          2   2568723.5  1881314    3256133  cudaDeviceSynchronize\n",
      "     0.0           119157          4     29789.3     8756      83701  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances   Average   Minimum  Maximum                     Name                    \n",
      " -------  ---------------  ---------  ---------  -------  -------  -------------------------------------------\n",
      "    52.0          2038711          3   679570.3   674994   685586  initWith(float, float*, int)               \n",
      "    48.0          1879355          1  1879355.0  1879355  1879355  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average   Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  --------  -------  -------  ---------------------------------\n",
      "   100.0         20446312          64  319473.6   319066   327673  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average   Minimum   Maximum               Operation            \n",
      " ----------  ----------  --------  --------  --------  ---------------------------------\n",
      " 131072.000          64  2048.000  2048.000  2048.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum              Name           \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------------------\n",
      "    67.4        432462389         28  15445085.3    47407  100129494  poll                      \n",
      "    21.4        137297552        689    199270.8     1025   39812725  ioctl                     \n",
      "     8.8         56259804         24   2344158.5    11370   20558742  sem_timedwait             \n",
      "     2.0         12907716         99    130381.0     1349    8569040  mmap                      \n",
      "     0.3          1613474         82     19676.5     4556      37369  open64                    \n",
      "     0.0           176229          5     35245.8    30201      39072  pthread_create            \n",
      "     0.0           165171          3     55057.0    53125      58329  fgets                     \n",
      "     0.0           117695         25      4707.8     1522      19709  fopen                     \n",
      "     0.0           104920         12      8743.3     4486      13743  write                     \n",
      "     0.0            95249          2     47624.5    31887      63362  sem_wait                  \n",
      "     0.0            39533         11      3593.9     1959       5685  munmap                    \n",
      "     0.0            30145         21      1435.5     1044       4768  fcntl                     \n",
      "     0.0            27326         18      1518.1     1035       4655  fclose                    \n",
      "     0.0            26611          5      5322.2     3156       8279  open                      \n",
      "     0.0            20486         13      1575.8     1012       2832  read                      \n",
      "     0.0            18848          4      4712.0     1470       9472  fgetc                     \n",
      "     0.0            13624          1     13624.0    13624      13624  pthread_rwlock_timedwrlock\n",
      "     0.0            11460          3      3820.0     2940       4899  fread                     \n",
      "     0.0            10116          2      5058.0     4590       5526  socket                    \n",
      "     0.0             9528          5      1905.6     1739       2092  mprotect                  \n",
      "     0.0             7475          1      7475.0     7475       7475  pipe2                     \n",
      "     0.0             5745          1      5745.0     5745       5745  connect                   \n",
      "     0.0             1994          1      1994.0     1994       1994  bind                      \n",
      "     0.0             1585          1      1585.0     1585       1585  listen                    \n",
      "\n",
      "Report file moved to \"/dli/task/report23.qdrep\"\n",
      "Report file moved to \"/dli/task/report23.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./prefetch-to-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this series of refactors to use asynchronous prefetching, you should see that there are fewer, but larger, memory transfers, and, that the kernel execution time is significantly decreased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "At this point in the lab, you are able to:\n",
    "\n",
    "- Use the Nsight Systems command line tool (**nsys**) to profile accelerated application performance.\n",
    "- Leverage an understanding of **Streaming Multiprocessors** to optimize execution configurations.\n",
    "- Understand the behavior of **Unified Memory** with regard to page faulting and data migrations.\n",
    "- Use **asynchronous memory prefetching** to reduce page faults and data migrations for increased performance.\n",
    "- Employ an iterative development cycle to rapidly accelerate and deploy applications.\n",
    "\n",
    "In order to consolidate your learning, and reinforce your ability to iteratively accelerate, optimize, and deploy applications, please proceed to this lab's final exercise. After completing it, for those of you with time and interest, please proceed to the *Advanced Content* section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Final Exercise: Iteratively Optimize an Accelerated SAXPY Application\n",
    "\n",
    "A basic accelerated SAXPY (Single Precision a\\*x+b) application has been provided for you [here](../edit/09-saxpy/01-saxpy.cu). It currently contains a couple of bugs that you will need to find and fix before you can successfully compile, run, and then profile it with `nsys profile`.\n",
    "\n",
    "After fixing the bugs and profiling the application, record the runtime of the `saxpy` kernel and then work *iteratively* to optimize the application, using `nsys profile` after each iteration to notice the effects of the code changes on kernel performance and UM behavior.\n",
    "\n",
    "Utilize the techniques from this lab. To support your learning, utilize [effortful retrieval](http://sites.gsu.edu/scholarlyteaching/effortful-retrieval/) whenever possible, rather than rushing to look up the specifics of techniques from earlier in the lesson.\n",
    "\n",
    "Your end goal is to profile an accurate `saxpy` kernel, without modifying `N`, to run in under *200us*. Check out [the solution](../edit/09-saxpy/solutions/02-saxpy-solution.cu) if you get stuck, and feel free to compile and profile it if you wish.\n",
    "\n",
    "### Only Bug Fixes\n",
    "\n",
    "```\n",
    "CUDA Kernel Statistics:\n",
    "\n",
    " Time(%)  Total Time (ns)  Instances   Average    Minimum   Maximum            Name          \n",
    " -------  ---------------  ---------  ----------  --------  --------  -----------------------\n",
    "   100.0         19127823          1  19127823.0  19127823  19127823  saxpy(int*, int*, int*)\n",
    "\n",
    "\n",
    "CUDA Memory Operation Statistics (by time):\n",
    "\n",
    " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
    " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
    "    99.7          9104061         580  15696.7     2111   166812  [CUDA Unified Memory memcpy HtoD]\n",
    "     0.3            24030           4   6007.5     1695    10560  [CUDA Unified Memory memcpy DtoH]\n",
    "\n",
    "\n",
    "CUDA Memory Operation Statistics (by size in KiB):\n",
    "\n",
    "   Total    Operations  Average  Minimum  Maximum               Operation            \n",
    " ---------  ----------  -------  -------  --------  ---------------------------------\n",
    " 49152.000         580   84.745    4.000  1000.000  [CUDA Unified Memory memcpy HtoD]\n",
    "   128.000           4   32.000    4.000    60.000  [CUDA Unified Memory memcpy DtoH]\n",
    "```\n",
    "\n",
    "With only bugs fixed: $19232333\\texttt{ns}$ = $19232\\mu s$\n",
    "\n",
    "I noticed the largest time consumer in terms of memory operations was HtoD, which also occupied half of the total runtime (9170163ns of the 19232333ns runtime of the `saxpy` kernel).\n",
    "We can speed this up using prefetching.\n",
    "\n",
    "### Memory Prefetch to Device\n",
    "\n",
    "```\n",
    "CUDA Kernel Statistics:\n",
    "\n",
    " Time(%)  Total Time (ns)  Instances  Average   Minimum  Maximum           Name          \n",
    " -------  ---------------  ---------  --------  -------  -------  -----------------------\n",
    "   100.0           194396          1  194396.0   194396   194396  saxpy(int*, int*, int*)\n",
    "\n",
    "\n",
    "CUDA Memory Operation Statistics (by time):\n",
    "\n",
    " Time(%)  Total Time (ns)  Operations  Average   Minimum  Maximum              Operation            \n",
    " -------  ---------------  ----------  --------  -------  -------  ---------------------------------\n",
    "    99.7          8199484          24  341645.2   339897   343577  [CUDA Unified Memory memcpy HtoD]\n",
    "     0.3            24030           4    6007.5     1695    10560  [CUDA Unified Memory memcpy DtoH]\n",
    "\n",
    "\n",
    "CUDA Memory Operation Statistics (by size in KiB):\n",
    "\n",
    "   Total    Operations  Average   Minimum   Maximum               Operation            \n",
    " ---------  ----------  --------  --------  --------  ---------------------------------\n",
    " 49152.000          24  2048.000  2048.000  2048.000  [CUDA Unified Memory memcpy HtoD]\n",
    "   128.000           4    32.000     4.000    60.000  [CUDA Unified Memory memcpy DtoH]\n",
    "```\n",
    "\n",
    "That little optimization alone was enough to get our kernel time down to $194396\\texttt{ns}$ = $194\\mu s$.\n",
    "Success!\n",
    "The number of memcpy operations is way down, with their average size way up.\n",
    "Grabbing massive pages is the way to go.\n",
    "\n",
    "### How Much Faster is the Solution Code?\n",
    "\n",
    "The only other optimization I could think of was optimizing the block size to try and match the number of stream multiprocessors.\n",
    "I was too lazy to implement it though, so I ran the solution and put the results below for comparison.\n",
    "\n",
    "```\n",
    "CUDA Kernel Statistics:\n",
    "\n",
    " Time(%)  Total Time (ns)  Instances  Average   Minimum  Maximum           Name          \n",
    " -------  ---------------  ---------  --------  -------  -------  -----------------------\n",
    "   100.0           197596          1  197596.0   197596   197596  saxpy(int*, int*, int*)\n",
    "\n",
    "\n",
    "CUDA Memory Operation Statistics (by time):\n",
    "\n",
    " Time(%)  Total Time (ns)  Operations  Average   Minimum  Maximum              Operation            \n",
    " -------  ---------------  ----------  --------  -------  -------  ---------------------------------\n",
    "    99.7          8180286          24  340845.3   339545   343641  [CUDA Unified Memory memcpy HtoD]\n",
    "     0.3            24733           4    6183.3     1695    10592  [CUDA Unified Memory memcpy DtoH]\n",
    "\n",
    "\n",
    "CUDA Memory Operation Statistics (by size in KiB):\n",
    "\n",
    "   Total    Operations  Average   Minimum   Maximum               Operation            \n",
    " ---------  ----------  --------  --------  --------  ---------------------------------\n",
    " 49152.000          24  2048.000  2048.000  2048.000  [CUDA Unified Memory memcpy HtoD]\n",
    "   128.000           4    32.000     4.000    60.000  [CUDA Unified Memory memcpy DtoH]\n",
    "```\n",
    "\n",
    "Not actually faster at all; most of our runtime was dominated by page faults/fetches anyways, and this micro-optimization clearly has a negligible benefit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09-saxpy/01-saxpy.cu(39): warning: variable \"multiProcessorCount\" was declared but never referenced\n",
      "\n",
      "Grid size: 16385, Block size: 256\n",
      "c[0] = 5, c[1] = 5, c[2] = 5, c[3] = 5, c[4] = 5, \n",
      "c[4194299] = 5, c[4194300] = 5, c[4194301] = 5, c[4194302] = 5, c[4194303] = 5, \n"
     ]
    }
   ],
   "source": [
    "!nvcc -o saxpy 09-saxpy/01-saxpy.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "Grid size: 16385, Block size: 256\n",
      "c[0] = 5, c[1] = 5, c[2] = 5, c[3] = 5, c[4] = 5, \n",
      "c[4194299] = 5, c[4194300] = 5, c[4194301] = 5, c[4194302] = 5, c[4194303] = 5, \n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-69a8-bdab-9936-0612.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-69a8-bdab-9936-0612.qdrep\"\n",
      "Exporting 1118 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-69a8-bdab-9936-0612.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum           Name         \n",
      " -------  ---------------  ---------  ----------  -------  ---------  ---------------------\n",
      "    95.0        239941835          3  79980611.7    26260  239868491  cudaMallocManaged    \n",
      "     2.2          5601260          1   5601260.0  5601260    5601260  cudaDeviceSynchronize\n",
      "     1.8          4424594          3   1474864.7   133192    3165682  cudaMemPrefetchAsync \n",
      "     1.0          2560147          3    853382.3   805784     905185  cudaFree             \n",
      "     0.0            41656          1     41656.0    41656      41656  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances  Average   Minimum  Maximum           Name          \n",
      " -------  ---------------  ---------  --------  -------  -------  -----------------------\n",
      "   100.0           194588          1  194588.0   194588   194588  saxpy(int*, int*, int*)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average   Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  --------  -------  -------  ---------------------------------\n",
      "    99.7          8178174          24  340757.3   339578   343226  [CUDA Unified Memory memcpy HtoD]\n",
      "     0.3            24061           4    6015.3     1663    10560  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total    Operations  Average   Minimum   Maximum               Operation            \n",
      " ---------  ----------  --------  --------  --------  ---------------------------------\n",
      " 49152.000          24  2048.000  2048.000  2048.000  [CUDA Unified Memory memcpy HtoD]\n",
      "   128.000           4    32.000     4.000    60.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum              Name           \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------------------\n",
      "    70.5        371639981         23  16158260.0    30780  100138848  poll                      \n",
      "    19.2        101279742        689    146995.3     1038   17358593  ioctl                     \n",
      "     8.5         44975078         17   2645592.8    17276   20589114  sem_timedwait             \n",
      "     1.0          5099275         98     52033.4     1506     813200  mmap                      \n",
      "     0.3          1826069          3    608689.7    13551    1776897  sem_wait                  \n",
      "     0.3          1498499         82     18274.4     4402      30257  open64                    \n",
      "     0.0           239663          5     47932.6    33764      66530  pthread_create            \n",
      "     0.0           166510          3     55503.3    52712      60204  fgets                     \n",
      "     0.0           130793         25      5231.7     1546      22432  fopen                     \n",
      "     0.0            93170         12      7764.2     4307      12130  write                     \n",
      "     0.0            34281          9      3809.0     2124       7414  munmap                    \n",
      "     0.0            33834          6      5639.0     1072      11350  fgetc                     \n",
      "     0.0            31593          5      6318.6     3404       9239  open                      \n",
      "     0.0            27217         18      1512.1     1087       4470  fclose                    \n",
      "     0.0            23800         14      1700.0     1012       3060  read                      \n",
      "     0.0            21760          2     10880.0     8221      13539  pthread_rwlock_timedwrlock\n",
      "     0.0            20119          2     10059.5     9248      10871  socket                    \n",
      "     0.0            16657          1     16657.0    16657      16657  bind                      \n",
      "     0.0            14292          9      1588.0     1027       5100  fcntl                     \n",
      "     0.0            11365          5      2273.0     1641       3022  mprotect                  \n",
      "     0.0             8758          2      4379.0     3872       4886  fread                     \n",
      "     0.0             8205          1      8205.0     8205       8205  pipe2                     \n",
      "     0.0             7975          1      7975.0     7975       7975  connect                   \n",
      "     0.0             1861          1      1861.0     1861       1861  listen                    \n",
      "\n",
      "Report file moved to \"/dli/task/report36.qdrep\"\n",
      "Report file moved to \"/dli/task/report36.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./saxpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c[0] = 5, c[1] = 5, c[2] = 5, c[3] = 5, c[4] = 5, \n",
      "c[4194299] = 5, c[4194300] = 5, c[4194301] = 5, c[4194302] = 5, c[4194303] = 5, \n",
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "c[0] = 5, c[1] = 5, c[2] = 5, c[3] = 5, c[4] = 5, \n",
      "c[4194299] = 5, c[4194300] = 5, c[4194301] = 5, c[4194302] = 5, c[4194303] = 5, \n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-ef43-2b9e-0ba0-f803.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-ef43-2b9e-0ba0-f803.qdrep\"\n",
      "Exporting 1104 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-ef43-2b9e-0ba0-f803.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum           Name         \n",
      " -------  ---------------  ---------  ----------  -------  ---------  ---------------------\n",
      "    94.7        225941762          3  75313920.7    23769  225881841  cudaMallocManaged    \n",
      "     2.4          5628791          1   5628791.0  5628791    5628791  cudaDeviceSynchronize\n",
      "     1.9          4447550          3   1482516.7   138810    3181586  cudaMemPrefetchAsync \n",
      "     1.1          2596413          3    865471.0   822973     900617  cudaFree             \n",
      "     0.0            42273          1     42273.0    42273      42273  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances  Average   Minimum  Maximum           Name          \n",
      " -------  ---------------  ---------  --------  -------  -------  -----------------------\n",
      "   100.0           197596          1  197596.0   197596   197596  saxpy(int*, int*, int*)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average   Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  --------  -------  -------  ---------------------------------\n",
      "    99.7          8180286          24  340845.3   339545   343641  [CUDA Unified Memory memcpy HtoD]\n",
      "     0.3            24733           4    6183.3     1695    10592  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total    Operations  Average   Minimum   Maximum               Operation            \n",
      " ---------  ----------  --------  --------  --------  ---------------------------------\n",
      " 49152.000          24  2048.000  2048.000  2048.000  [CUDA Unified Memory memcpy HtoD]\n",
      "   128.000           4    32.000     4.000    60.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum              Name           \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------------------\n",
      "    74.4        371347918         23  16145561.7    29710  100137810  poll                      \n",
      "    17.5         87242195        677    128865.9     1060   17609155  ioctl                     \n",
      "     6.2         31115079         16   1944692.4    10266   20530911  sem_timedwait             \n",
      "     1.0          5167806         98     52732.7     1504     832072  mmap                      \n",
      "     0.4          1872190          3    624063.3    13904    1816731  sem_wait                  \n",
      "     0.3          1521319         82     18552.7     5069      30956  open64                    \n",
      "     0.0           238727          5     47745.4    31869      66551  pthread_create            \n",
      "     0.0           167186          3     55728.7    52501      60507  fgets                     \n",
      "     0.0           136340         25      5453.6     1536      24256  fopen                     \n",
      "     0.0            94827         12      7902.3     4349      14475  write                     \n",
      "     0.0            42026          2     21013.0    10313      31713  socket                    \n",
      "     0.0            36042          9      4004.7     1929       7753  munmap                    \n",
      "     0.0            33397          6      5566.2     1268      11126  fgetc                     \n",
      "     0.0            33102          5      6620.4     3440       8641  open                      \n",
      "     0.0            27827         18      1545.9     1042       4870  fclose                    \n",
      "     0.0            25055         14      1789.6     1019       3900  read                      \n",
      "     0.0            14035          9      1559.4     1015       4593  fcntl                     \n",
      "     0.0            13538          5      2707.6     1637       5102  mprotect                  \n",
      "     0.0             9800          1      9800.0     9800       9800  pthread_rwlock_timedwrlock\n",
      "     0.0             8899          1      8899.0     8899       8899  pipe2                     \n",
      "     0.0             8870          2      4435.0     3743       5127  fread                     \n",
      "     0.0             8674          1      8674.0     8674       8674  connect                   \n",
      "     0.0             3366          1      3366.0     3366       3366  bind                      \n",
      "     0.0             1710          1      1710.0     1710       1710  listen                    \n",
      "\n",
      "Report file moved to \"/dli/task/report38.qdrep\"\n",
      "Report file moved to \"/dli/task/report38.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o saxpy-solution 09-saxpy/solutions/02-saxpy-solution.cu -run\n",
    "!nsys profile --stats=true ./saxpy-solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
