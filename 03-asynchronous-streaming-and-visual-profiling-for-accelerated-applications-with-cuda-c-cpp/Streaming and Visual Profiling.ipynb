{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><div align=\"center\">Asynchronous Streaming, and Visual Profiling with CUDA C/C++</div></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CUDA](./images/CUDA_Logo.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CUDA toolkit ships with the **Nsight Systems**, a powerful GUI application to support the development of accelerated CUDA applications. Nsight Systems generates a graphical timeline of an accelerated application, with detailed information about CUDA API calls, kernel execution, memory activity, and the use of **CUDA streams**.\n",
    "\n",
    "In this lab, you will be using the Nsight Systems timeline to guide you in optimizing accelerated applications. Additionally, you will learn some intermediate CUDA programming techniques to support your work: **unmanaged memory allocation and migration**; **pinning**, or **page-locking** host memory; and **non-default concurrent CUDA streams**.\n",
    "\n",
    "At the end of this lab, you will be presented with an assessment, to accelerate and optimize a simple n-body particle simulator, which will allow you to demonstrate the skills you have developed during this course. Those of you who are able to accelerate the simulator while maintaining its correctness, will be granted a certification as proof of your competency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prerequisites\n",
    "\n",
    "To get the most out of this lab you should already be able to:\n",
    "\n",
    "- Write, compile, and run C/C++ programs that both call CPU functions and launch GPU kernels.\n",
    "- Control parallel thread hierarchy using execution configuration.\n",
    "- Refactor serial loops to execute their iterations in parallel on a GPU.\n",
    "- Allocate and free CUDA Unified Memory.\n",
    "- Understand the behavior of Unified Memory with regard to page faulting and data migrations.\n",
    "- Use asynchronous memory prefetching to reduce page faults and data migrations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "By the time you complete this lab you will be able to:\n",
    "\n",
    "- Use **Nsight Systems** to visually profile the timeline of GPU-accelerated CUDA applications.\n",
    "- Use Nsight Systems to identify, and exploit, optimization opportunities in GPU-accelerated CUDA applications.\n",
    "- Utilize CUDA streams for concurrent kernel execution in accelerated applications.\n",
    "- (**Optional Advanced Content**) Use manual device memory allocation, including allocating pinned memory, in order to asynchronously transfer data in concurrent CUDA streams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Running Nsight Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this interactive lab environment, we have set up a remote desktop you can access from your browser, where you will be able to launch and use Nsight Systems.\n",
    "\n",
    "You will begin by creating a report file for an already-existing vector addition program, after which you will be walked through a series of steps to open this report file in Nsight Systems, and to make the visual experience nice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Report File\n",
    "\n",
    "[`01-vector-add.cu`](../edit/01-vector-add/01-vector-add.cu) (<-------- click on these links to source files to edit them in the browser) contains a working, accelerated, vector addition application. Use the code execution cell directly below (you can execute it, and any of the code execution cells in this lab by `CTRL` + clicking it) to compile and run it. You should see a message printed that indicates it was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o vector-add-no-prefetch 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use `nsys profile --stats=true` to create a report file that you will be able to open in the Nsight Systems visual profiler. Here we use the `-o` flag to give the report file a memorable name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-9608-d01a-7bf4-a60e.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-9608-d01a-7bf4-a60e.qdrep\"\n",
      "Exporting 10127 events: [=================================================100%][==10%                                                ]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-9608-d01a-7bf4-a60e.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls    Average     Minimum    Maximum           Name         \n",
      " -------  ---------------  ---------  -----------  ---------  ---------  ---------------------\n",
      "    67.6        313874327          3  104624775.7      17436  313814291  cudaMallocManaged    \n",
      "    27.3        126695354          1  126695354.0  126695354  126695354  cudaDeviceSynchronize\n",
      "     5.1         23610075          3    7870025.0    6856881    9817129  cudaFree             \n",
      "     0.0            51956          1      51956.0      51956      51956  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances    Average     Minimum    Maximum                      Name                    \n",
      " -------  ---------------  ---------  -----------  ---------  ---------  -------------------------------------------\n",
      "   100.0        126685717          1  126685717.0  126685717  126685717  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
      "    77.7         71373877        8191   8713.7     2174   128413  [CUDA Unified Memory memcpy HtoD]\n",
      "    22.3         20469670         756  27076.3     1599   160894  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average  Minimum  Maximum               Operation            \n",
      " ----------  ----------  -------  -------  --------  ---------------------------------\n",
      " 348672.000        8191   42.568    4.000   764.000  [CUDA Unified Memory memcpy HtoD]\n",
      " 129024.000         756  170.667    4.000  1020.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum        Name     \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------\n",
      "    82.4       1444232119         75  19256428.3     3516  100126858  poll          \n",
      "     8.4        147324061         65   2266524.0     9438   20623108  sem_timedwait \n",
      "     7.4        130274019        679    191861.6     1030   33877296  ioctl         \n",
      "     1.6         27239370         94    289780.5     1468    9762232  mmap          \n",
      "     0.1          1762968         82     21499.6    10945      47245  open64        \n",
      "     0.0           365512          3    121837.3   118206     126168  fgets         \n",
      "     0.0           283036          4     70759.0    55184      81731  pthread_create\n",
      "     0.0           213036         25      8521.4     2973      32490  fopen         \n",
      "     0.0            89888         11      8171.6     4246      14352  write         \n",
      "     0.0            64923          5     12984.6     7540      18195  open          \n",
      "     0.0            54611         12      4550.9     1935       9058  munmap        \n",
      "     0.0            49504          6      8250.7     1869      25984  fgetc         \n",
      "     0.0            44206         18      2455.9     1577       4455  fclose        \n",
      "     0.0            39194         26      1507.5     1023       5930  fcntl         \n",
      "     0.0            33915          2     16957.5    15444      18471  socket        \n",
      "     0.0            24917         12      2076.4     1072       5343  read          \n",
      "     0.0            18247          3      6082.3     1819      11224  fread         \n",
      "     0.0            15574          1     15574.0    15574      15574  connect       \n",
      "     0.0             7082          1      7082.0     7082       7082  pipe2         \n",
      "     0.0             5684          1      5684.0     5684       5684  bind          \n",
      "     0.0             3449          1      3449.0     3449       3449  listen        \n",
      "\n",
      "Report file moved to \"/dli/task/vector-add-no-prefetch-report.qdrep\"\n",
      "Report file moved to \"/dli/task/vector-add-no-prefetch-report.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true -o vector-add-no-prefetch-report ./vector-add-no-prefetch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open the Remote Desktop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to generate a link to the remote desktop. Then, read the instructions that follow in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "var port = ((window.location.port == 80) ? \"\" : (\":\"+window.location.port));\n",
       "var url = 'http://' + window.location.hostname + port + '/nsight/vnc.html?resize=scale';\n",
       "let a = document.createElement('a');\n",
       "a.setAttribute('href', url)\n",
       "a.setAttribute('target', '_blank')\n",
       "a.innerText = 'Click to open remote desktop'\n",
       "element.append(a);\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%js\n",
    "var port = ((window.location.port == 80) ? \"\" : (\":\"+window.location.port));\n",
    "var url = 'http://' + window.location.hostname + port + '/nsight/vnc.html?resize=scale';\n",
    "let a = document.createElement('a');\n",
    "a.setAttribute('href', url)\n",
    "a.setAttribute('target', '_blank')\n",
    "a.innerText = 'Click to open remote desktop'\n",
    "element.append(a);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After clicking the _Connect_ button you will be asked for a password, which is `nvidia`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open Nsight Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To open Nsight Systems, double-click the \"NVIDIA Nsight Systems\" icon on the remote desktop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![open nsight](images/open-nsight-sys.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable Usage Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When prompted, click \"Yes\" to enable usage reporting:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![enable usage](images/enable_usage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select GPU Rows on Top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When prompted, select _GPU Rows on Top_ and then click _Okay_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gpu)_rows_on_top](images/gpu_on_top.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open the Report File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open this report file by visiting _File_ -> _Open_ from the Nsight Systems menu and select `vector-add-no-prefetch-report.qdrep`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![open-report](images/open-report.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ignore Warnings/Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can close and ignore any warnings or errors you see, which are just a result of our particular remote desktop environment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ignore errors](images/ignore-error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make More Room for the Timelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make your experience nicer, full-screen the profiler, close the _Project Explorer_ and hide the *Events View*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![make nice](images/make-nice.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your screen should now look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![now nice](images/now-nice.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expand the CUDA Unified Memory Timelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, expand the _CUDA_ -> _Unified memory_ and _Context_ timelines, and close the _Threads_ timelines:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![open memory](images/open-memory.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe Many Memory Transfers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a glance you can see that your application is taking about 1 second to run, and that also, during the time when the `addVectorsInto` kernel is running, that there is a lot of UM memory activity:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![memory and kernel](images/memory-and-kernel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zoom into the memory timelines to see more clearly all the small memory transfers being caused by the on-demand memory page faults. A couple tips:\n",
    "\n",
    "1. You can zoom in and out at any point of the timeline by holding `CTRL` while scrolling your mouse/trackpad\n",
    "2. You can zoom into any section by click + dragging a rectangle around it, and then selecting _Zoom in_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of zooming in to see the many small memory transfers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![many transfers](images/many-transfers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Comparing Code Refactors Iteratively with Nsight Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have Nsight Systems up and running and are comfortable moving around the timelines, you will be profiling a series of programs that were iteratively improved using techniques already familiar to you. Each time you profile, information in the timeline will give information supporting how you should next modify your code. Doing this will further increase your understanding of how various CUDA programming techniques affect application performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Compare the Timelines of Prefetching vs. Non-Prefetching\n",
    "\n",
    "[`01-vector-add-prefetch-solution.cu`](../edit/01-vector-add/solutions/01-vector-add-prefetch-solution.cu) refactors the vector addition application from above so that the 3 vectors needed by its `addVectorsInto` kernel are asynchronously prefetched to the active GPU device prior to launching the kernel (using [`cudaMemPrefetchAsync`](http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1ge8dc9199943d421bc8bc7f473df12e42)). Open the source code and identify where in the application these changes were made.\n",
    "\n",
    "After reviewing the changes, compile and run the refactored application using the code execution cell directly below. You should see its success message printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o vector-add-prefetch 01-vector-add/solutions/01-vector-add-prefetch-solution.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a report file for this version of the application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-6140-321f-1096-33cb.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-6140-321f-1096-33cb.qdrep\"\n",
      "Exporting 2109 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-6140-321f-1096-33cb.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum    Maximum           Name         \n",
      " -------  ---------------  ---------  ----------  --------  ---------  ---------------------\n",
      "    72.3        238119453          3  79373151.0     26573  238045300  cudaMallocManaged    \n",
      "    11.6         38168657          1  38168657.0  38168657   38168657  cudaDeviceSynchronize\n",
      "     9.5         31317039          3  10439013.0    136588   22546530  cudaMemPrefetchAsync \n",
      "     6.6         21897810          3   7299270.0   6162116    8227054  cudaFree             \n",
      "     0.0            40203          1     40203.0     40203      40203  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances   Average   Minimum  Maximum                     Name                    \n",
      " -------  ---------------  ---------  ---------  -------  -------  -------------------------------------------\n",
      "   100.0          1690815          1  1690815.0  1690815  1690815  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average   Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  --------  -------  -------  ---------------------------------\n",
      "    75.9         65517712         192  341238.1   339545   344633  [CUDA Unified Memory memcpy HtoD]\n",
      "    24.1         20780345         768   27057.7     1631   159901  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average   Minimum   Maximum               Operation            \n",
      " ----------  ----------  --------  --------  --------  ---------------------------------\n",
      " 393216.000         192  2048.000  2048.000  2048.000  [CUDA Unified Memory memcpy HtoD]\n",
      " 131072.000         768   170.667     4.000  1020.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum        Name     \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------\n",
      "    78.9       1175108119         67  17538927.1    21968  100139293  poll          \n",
      "     9.6        142705461        678    210480.0     1003   22415552  ioctl         \n",
      "     8.8        131518404         57   2307340.4    14268   20865260  sem_timedwait \n",
      "     1.6         24319310         94    258716.1     1185    8165704  mmap          \n",
      "     0.9         13742012          2   6871006.0    35521   13706491  sem_wait      \n",
      "     0.1          1451553         82     17701.9     4514      30284  open64        \n",
      "     0.0           227262          5     45452.4    31966      68162  pthread_create\n",
      "     0.0           160623          3     53541.0    51339      57409  fgets         \n",
      "     0.0           113989         25      4559.6     1476      21780  fopen         \n",
      "     0.0            83017         12      6918.1     4201      11616  write         \n",
      "     0.0            42241         11      3840.1     1698       5522  munmap        \n",
      "     0.0            33518          5      6703.6     1024      23103  fgetc         \n",
      "     0.0            30043          5      6008.6     3544      10346  open          \n",
      "     0.0            25668         18      1426.0     1009       4227  fclose        \n",
      "     0.0            21125         14      1508.9     1092       2282  read          \n",
      "     0.0            11551          2      5775.5     5309       6242  socket        \n",
      "     0.0             9482          3      3160.7     1710       3916  fread         \n",
      "     0.0             9185          1      9185.0     9185       9185  pipe2         \n",
      "     0.0             8428          5      1685.6     1066       4092  fcntl         \n",
      "     0.0             6066          1      6066.0     6066       6066  connect       \n",
      "     0.0             2500          1      2500.0     2500       2500  bind          \n",
      "     0.0             1460          1      1460.0     1460       1460  listen        \n",
      "\n",
      "Report file moved to \"/dli/task/vector-add-prefetch-report.qdrep\"\n",
      "Report file moved to \"/dli/task/vector-add-prefetch-report.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true -o vector-add-prefetch-report ./vector-add-prefetch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the report in Nsight Systems, leaving the previous report open for comparison.\n",
    "\n",
    "- How does the execution time compare to that of the `addVectorsInto` kernel prior to adding asynchronous prefetching?\n",
    "    - Execution time drops from $126685717\\texttt{ns}$ to $1690815\\texttt{ns}$, a reduction of almost a factor of 20.\n",
    "    - HtoD memcpy ops took $71373877\\texttt{ns}$ originally, over half of the original runtime\n",
    "- Locate `cudaMemPrefetchAsync` in the *CUDA API* section of the timeline.\n",
    "- How have the memory transfers changed?\n",
    "    - Originally a sequence of page faults, then variable size speculative prefetches. Now a sequence of large prefetches before running the kernel page fault-free"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise: Profile Refactor with Launch Init in Kernel\n",
    "\n",
    "In the previous iteration of the vector addition application, the vector data is being initialized on the CPU, and therefore needs to be migrated to the GPU before the `addVectorsInto` kernel can operate on it.\n",
    "\n",
    "The next iteration of the application, [01-init-kernel-solution.cu](../edit/02-init-kernel/solutions/01-init-kernel-solution.cu), the application has been refactored to initialize the data in parallel on the GPU.\n",
    "\n",
    "Since the initialization now takes place on the GPU, prefetching has been done prior to initialization, rather than prior to the vector addition work. Review the source code to identify where these changes have been made.\n",
    "\n",
    "After reviewing the changes, compile and run the refactored application using the code execution cell directly below. You should see its success message printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o init-kernel 02-init-kernel/solutions/01-init-kernel-solution.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a report file for this version of the application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-0f55-1c61-a82b-253f.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-0f55-1c61-a82b-253f.qdrep\"\n",
      "Exporting 1879 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-0f55-1c61-a82b-253f.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum           Name         \n",
      " -------  ---------------  ---------  ----------  -------  ---------  ---------------------\n",
      "    91.5        264664317          3  88221439.0    37485  264514734  cudaMallocManaged    \n",
      "     5.9         16953804          3   5651268.0   802144   15237275  cudaFree             \n",
      "     1.4          4026920          3   1342306.7  1285967    1435488  cudaMemPrefetchAsync \n",
      "     1.2          3533141          1   3533141.0  3533141    3533141  cudaDeviceSynchronize\n",
      "     0.0            99980          4     24995.0    10481      63308  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances   Average   Minimum  Maximum                     Name                    \n",
      " -------  ---------------  ---------  ---------  -------  -------  -------------------------------------------\n",
      "    52.2          1864419          3   621473.0   620854   622646  initWith(float, float*, int)               \n",
      "    47.8          1705733          1  1705733.0  1705733  1705733  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
      "   100.0         20683329         768  26931.4     1599   155774  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average  Minimum  Maximum               Operation            \n",
      " ----------  ----------  -------  -------  --------  ---------------------------------\n",
      " 131072.000         768  170.667    4.000  1020.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum        Name     \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------\n",
      "    77.9        742656214         40  18566405.4    21032  100135042  poll          \n",
      "    11.1        106263626        682    155811.8     1076   17469583  ioctl         \n",
      "     8.6         82338545         34   2421721.9    11247   20612967  sem_timedwait \n",
      "     2.1         19823469         94    210888.0     1582   15181997  mmap          \n",
      "     0.2          1875160         82     22867.8    16877      35563  open64        \n",
      "     0.0           229077          3     76359.0    68699      80700  fgets         \n",
      "     0.0           164467         25      6578.7     2064      21633  fopen         \n",
      "     0.0           162579          4     40644.8    31584      48461  pthread_create\n",
      "     0.0            83103         11      7554.8     4404      12992  write         \n",
      "     0.0            54622         13      4201.7     2100       7853  munmap        \n",
      "     0.0            40368          5      8073.6     5732      11863  open          \n",
      "     0.0            36368         18      2020.4     1227       4595  fclose        \n",
      "     0.0            23428          6      3904.7     1028      11202  fgetc         \n",
      "     0.0            21744         12      1812.0     1036       3590  read          \n",
      "     0.0            21506          4      5376.5     2472       9663  fread         \n",
      "     0.0            17134          2      8567.0     6705      10429  socket        \n",
      "     0.0            14032          9      1559.1     1001       5003  fcntl         \n",
      "     0.0             9544          1      9544.0     9544       9544  connect       \n",
      "     0.0             7599          1      7599.0     7599       7599  pipe2         \n",
      "     0.0             2606          1      2606.0     2606       2606  bind          \n",
      "     0.0             1572          1      1572.0     1572       1572  listen        \n",
      "\n",
      "Report file moved to \"/dli/task/init-kernel-report.qdrep\"\n",
      "Report file moved to \"/dli/task/init-kernel-report.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true -o init-kernel-report ./init-kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the new report file in Nsight Systems and do the following:\n",
    "\n",
    "- Compare the application and `addVectorsInto` run times to the previous version of the application, how did they change?\n",
    "    - `addVectorsInto` kernel time hansn't changed (1690815ns vs 1705733ns), which is to be expected, since prefetching saved us time before. But there are now no HtoD memcpy operations, meaning we saved the 65517712ns spent prefetching the initialized vectors to the device.\n",
    "- Look at the *Kernels* section of the timeline. Which of the two kernels (`addVectorsInto` and the initialization kernel) is taking up the majority of the time on the GPU?\n",
    "    - `addVectorsInto` technically takes three times as long as `initWith`, but I suspect this is because it has to perform two load operations, then an addition, then store operation per index. Compare this with `initWith`, which only needs to perform a store operation. But since `initWith` must be performed three times, the runtimes come out to about the same\n",
    "- Which of the following does your application contain?\n",
    "  - Data Migration (HtoD): No\n",
    "  - Data Migration (DtoH): Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Profile Refactor with Asynchronous Prefetch Back to the Host\n",
    "\n",
    "Currently, the vector addition application verifies the work of the vector addition kernel on the host. The next refactor of the application, [01-prefetch-check-solution.cu](../edit/04-prefetch-check/solutions/01-prefetch-check-solution.cu), asynchronously prefetches the data back to the host for verification.\n",
    "\n",
    "After reviewing the changes, compile and run the refactored application using the code execution cell directly below. You should see its success message printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o prefetch-to-host 04-prefetch-check/solutions/01-prefetch-check-solution.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a report file for this version of the application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-dbc9-4c7d-24d0-dab9.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-dbc9-4c7d-24d0-dab9.qdrep\"\n",
      "Exporting 1157 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-dbc9-4c7d-24d0-dab9.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum           Name         \n",
      " -------  ---------------  ---------  ----------  -------  ---------  ---------------------\n",
      "    79.8        247723965          3  82574655.0    17985  247661075  cudaMallocManaged    \n",
      "    13.6         42342093          4  10585523.3    98732   40881015  cudaMemPrefetchAsync \n",
      "     4.9         15338539          3   5112846.3  1042227   13111660  cudaFree             \n",
      "     1.6          4826718          1   4826718.0  4826718    4826718  cudaDeviceSynchronize\n",
      "     0.0            63210          4     15802.5     7809      36019  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances   Average   Minimum  Maximum                     Name                    \n",
      " -------  ---------------  ---------  ---------  -------  -------  -------------------------------------------\n",
      "    52.4          1874557          3   624852.3   619956   627989  initWith(float, float*, int)               \n",
      "    47.6          1704000          1  1704000.0  1704000  1704000  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average   Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  --------  -------  -------  ---------------------------------\n",
      "   100.0         19914632          64  311166.1   310906   312154  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average   Minimum   Maximum               Operation            \n",
      " ----------  ----------  --------  --------  --------  ---------------------------------\n",
      " 131072.000          64  2048.000  2048.000  2048.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum        Name     \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------\n",
      "    71.6        532325970         29  18356067.9    22556  100122480  poll          \n",
      "    19.6        145817995        683    213496.3     1004   40813327  ioctl         \n",
      "     6.0         44417670         22   2018985.0    14973   20873691  sem_timedwait \n",
      "     2.4         17986069         94    191341.2     1365   13044647  mmap          \n",
      "     0.2          1532844         82     18693.2     9807      34652  open64        \n",
      "     0.0           209689          5     41937.8    31912      48913  pthread_create\n",
      "     0.0           205240          3     68413.3    58895      74282  fgets         \n",
      "     0.0           144854         25      5794.2     1644      21811  fopen         \n",
      "     0.0            89853          2     44926.5    33771      56082  sem_wait      \n",
      "     0.0            87078         12      7256.5     4345      10595  write         \n",
      "     0.0            52383         11      4762.1     1680      11547  munmap        \n",
      "     0.0            48359         18      2686.6     1185      17558  fclose        \n",
      "     0.0            37178          5      7435.6     5077      11859  open          \n",
      "     0.0            31790          7      4541.4     1007      10524  fgetc         \n",
      "     0.0            26179         14      1869.9     1095       3529  read          \n",
      "     0.0            15733          2      7866.5     6325       9408  socket        \n",
      "     0.0            11805          4      2951.3     1741       4340  fread         \n",
      "     0.0            11416          7      1630.9     1043       4381  fcntl         \n",
      "     0.0             7831          1      7831.0     7831       7831  connect       \n",
      "     0.0             7201          1      7201.0     7201       7201  pipe2         \n",
      "     0.0             2459          1      2459.0     2459       2459  bind          \n",
      "     0.0             1600          1      1600.0     1600       1600  listen        \n",
      "\n",
      "Report file moved to \"/dli/task/prefetch-to-host-report.qdrep\"\n",
      "Report file moved to \"/dli/task/prefetch-to-host-report.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true -o prefetch-to-host-report ./prefetch-to-host"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open this report file in Nsight Systems, and do the following:\n",
    "\n",
    "- Use the *Unified Memory* section of the timeline to compare and contrast the *Data Migration (DtoH)* events before and after adding prefetching back to the CPU.\n",
    "    - DtoH from 20683329ns to 19914632ns (essentially the same)\n",
    "    - Like our initial prefetching, we no longer have sequences of page faults and speculative prefetching, but instead a stream of large prefetches. I'm kind of surprised to see the performance is so similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Concurrent CUDA Streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now going to learn about a new concept, **CUDA Streams**. After an introduction to them, you will return to using Nsight Systems to better evaluate their impact on your application's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following slides present upcoming material visually, at a high level. Click through the slides before moving on to more detailed coverage of their topics in following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div align=\"center\"><iframe src=\"https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-01-V1/embedded/task3/NVVP-Streams-1.pptx\" width=\"800px\" height=\"500px\" frameborder=\"0\"></iframe></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "\n",
    "<div align=\"center\"><iframe src=\"https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-01-V1/embedded/task3/NVVP-Streams-1.pptx\" width=\"800px\" height=\"500px\" frameborder=\"0\"></iframe></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In CUDA programming, a **stream** is a series of commands that execute in order. In CUDA applications, kernel execution, as well as some memory transfers, occur within CUDA streams. Up until this point in time, you have not been interacting explicitly with CUDA streams, but in fact, your CUDA code has been executing its kernels inside of a stream called *the default stream*.\n",
    "\n",
    "CUDA programmers can create and utilize non-default CUDA streams in addition to the default stream, and in doing so, perform multiple operations, such as executing multiple kernels, concurrently, in different streams. Using multiple streams can add an additional layer of parallelization to your accelerated applications, and offers many more opportunities for application optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rules Governing the Behavior of CUDA Streams\n",
    "\n",
    "There are a few rules, concerning the behavior of CUDA streams, that should be learned in order to utilize them effectively:\n",
    "\n",
    "- Operations within a given stream occur in order.\n",
    "- Operations in different non-default streams are not guaranteed to operate in any specific order relative to each other.\n",
    "- The default stream is blocking and will both wait for all other streams to complete before running, and, will block other streams from running until it completes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating, Utilizing, and Destroying Non-Default CUDA Streams\n",
    "\n",
    "The following code snippet demonstrates how to create, utilize, and destroy a non-default CUDA stream. You will note, that to launch a CUDA kernel in a non-default CUDA stream, the stream must be passed as the optional 4th argument of the execution configuration. Up until now you have only utilized the first 2 arguments of the execution configuration:\n",
    "\n",
    "```cpp\n",
    "cudaStream_t stream;       // CUDA streams are of type `cudaStream_t`.\n",
    "cudaStreamCreate(&stream); // Note that a pointer must be passed to `cudaCreateStream`.\n",
    "\n",
    "someKernel<<<number_of_blocks, threads_per_block, 0, stream>>>(); // `stream` is passed as 4th EC argument.\n",
    "\n",
    "cudaStreamDestroy(stream); // Note that a value, not a pointer, is passed to `cudaDestroyStream`.\n",
    "```\n",
    "\n",
    "Outside the scope of this lab, but worth mentioning, is the optional 3rd argument of the execution configuration. This argument allows programmers to supply the number of bytes in **shared memory** (an advanced topic that will not be covered presently) to be dynamically allocated per block for this kernel launch. The default number of bytes allocated to shared memory per block is `0`, and for the remainder of the lab, you will be passing `0` as this value, in order to expose the 4th argument, which is of immediate interest:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Predict Default Stream Behavior\n",
    "\n",
    "The [01-print-numbers](../edit/05-stream-intro/01-print-numbers.cu) application has a very simple `printNumber` kernel which accepts an integer and prints it. The kernel is only being executed with a single thread inside a single block. However, it is being executed 5 times, using a for-loop, and passing each launch the number of the for-loop's iteration.\n",
    "\n",
    "Compile and run [01-print-numbers](../edit/05-stream-intro/01-print-numbers.cu) using the code execution block below. You should see the numbers `0` through `4` printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\r\n",
      "1\r\n",
      "2\r\n",
      "3\r\n",
      "4\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o print-numbers 05-stream-intro/01-print-numbers.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing that by default kernels are executed in the default stream, would you expect that the 5 launches of the `print-numbers` program executed serially, or in parallel? You should be able to mention two features of the default stream to support your answer. Create a report file in the cell below and open it in Nsight Systems to confirm your answer.\n",
    "\n",
    "**Answer**: They'll execute serially, since each call to `printNumbers` spawns a new kernel, we're by default in the default stream, and any single stream executes kernels sequentially. Confirmed in Nsight Systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-3264-c55b-4c03-5892.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-3264-c55b-4c03-5892.qdrep\"\n",
      "Exporting 1054 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-3264-c55b-4c03-5892.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum           Name         \n",
      " -------  ---------------  ---------  ----------  -------  ---------  ---------------------\n",
      "    99.9        255049473          5  51009894.6     4214  255028798  cudaLaunchKernel     \n",
      "     0.1           272982          1    272982.0   272982     272982  cudaDeviceSynchronize\n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances  Average  Minimum  Maximum        Name      \n",
      " -------  ---------------  ---------  -------  -------  -------  ----------------\n",
      "   100.0           274172          5  54834.4    53056    61503  printNumber(int)\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum  Maximum        Name     \n",
      " -------  ---------------  ---------  ----------  -------  --------  --------------\n",
      "    65.7        253713431         14  18122387.9    31686  80103416  poll          \n",
      "    32.5        125477046        672    186721.8     1049  20985095  ioctl         \n",
      "     0.9          3652872         87     41987.0     1907   1149316  mmap          \n",
      "     0.5          1930475         82     23542.4     7389     42921  open64        \n",
      "     0.1           570537          8     71317.1    10416    362351  sem_timedwait \n",
      "     0.1           219210          4     54802.5    50006     65633  pthread_create\n",
      "     0.0           190444         12     15870.3     4360     44340  write         \n",
      "     0.0           187128          3     62376.0    54635     72730  fgets         \n",
      "     0.0           169494         25      6779.8     1613     25377  fopen         \n",
      "     0.0            45517         32      1422.4     1003      6284  fcntl         \n",
      "     0.0            42101          5      8420.2     4008     13113  open          \n",
      "     0.0            36834          7      5262.0     1429     11378  munmap        \n",
      "     0.0            32313         18      1795.2     1130      5059  fclose        \n",
      "     0.0            29245          6      4874.2     1043     13127  fgetc         \n",
      "     0.0            26979         14      1927.1     1027      3092  read          \n",
      "     0.0            19879          2      9939.5     9411     10468  socket        \n",
      "     0.0            18556          2      9278.0     5870     12686  fread         \n",
      "     0.0            17349          1     17349.0    17349     17349  sem_wait      \n",
      "     0.0             8919          1      8919.0     8919      8919  connect       \n",
      "     0.0             8813          1      8813.0     8813      8813  pipe2         \n",
      "     0.0             3763          1      3763.0     3763      3763  bind          \n",
      "     0.0             2064          1      2064.0     2064      2064  listen        \n",
      "\n",
      "Report file moved to \"/dli/task/print-numbers-report.qdrep\"\n",
      "Report file moved to \"/dli/task/print-numbers-report.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true -o print-numbers-report ./print-numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Implement Concurrent CUDA Streams\n",
    "\n",
    "Both because all 5 kernel launches occurred in the same stream, you should not be surprised to have seen that the 5 kernels executed serially. Additionally you could make the case that because the default stream is blocking, each launch of the kernel would wait to complete before the next launch, and this is also true.\n",
    "\n",
    "Refactor [01-print-numbers](../edit/05-stream-intro/01-print-numbers.cu) so that each kernel launch occurs in its own non-default stream. Be sure to destroy the streams you create after they are no longer needed. Compile and run the refactored code with the code execution cell directly below. You should still see the numbers `0` through `4` printed, though not necessarily in ascending order. Refer to [the solution](../edit/05-stream-intro/solutions/01-print-numbers-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\r\n",
      "1\r\n",
      "2\r\n",
      "3\r\n",
      "4\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o print-numbers-in-streams 05-stream-intro/01-print-numbers.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you are using 5 different non-default streams for each of the 5 kernel launches, do you expect that they will run serially or in parallel? In addition to what you now know about streams, take into account how trivial the `printNumber` kernel is, meaning, even if you predict parallel runs, will the speed at which one kernel will complete allow for complete overlap?\n",
    "\n",
    "**Answer**: I expect them to run in parallel, since each stream can run independently as long as they're not on the default stream. Each kernel is so simple, though, that I don't much overlap.\n",
    "\n",
    "After hypothesizing, open a new report file in Nsight Systems to view its actual behavior. You should notice that now, there are additional rows in the _CUDA_ section for each of the non-default streams you created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-c812-a391-ac54-62a0.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-c812-a391-ac54-62a0.qdrep\"\n",
      "Exporting 1077 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-c812-a391-ac54-62a0.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum           Name         \n",
      " -------  ---------------  ---------  ----------  -------  ---------  ---------------------\n",
      "    99.9        296319563          5  59263912.6     2120  296309977  cudaStreamCreate     \n",
      "     0.0            92631          5     18526.2     6295      60952  cudaLaunchKernel     \n",
      "     0.0            57898          1     57898.0    57898      57898  cudaDeviceSynchronize\n",
      "     0.0            21722          5      4344.4     2626      10806  cudaStreamDestroy    \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances  Average  Minimum  Maximum        Name      \n",
      " -------  ---------------  ---------  -------  -------  -------  ----------------\n",
      "   100.0           299323          5  59864.6    56671    64063  printNumber(int)\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum  Maximum        Name     \n",
      " -------  ---------------  ---------  ----------  -------  --------  --------------\n",
      "    65.5        302139506         15  20142633.7    25105  94308072  poll          \n",
      "    32.5        150152698        672    223441.5     1030  36180039  ioctl         \n",
      "     1.0          4774897         87     54883.9     2558   1910295  mmap          \n",
      "     0.5          2103021         82     25646.6    12130     58176  open64        \n",
      "     0.2           706766         11     64251.5    16985    380853  sem_timedwait \n",
      "     0.1           370447          3    123482.3   118333    130043  fgets         \n",
      "     0.1           329012          4     82253.0    55462    104204  pthread_create\n",
      "     0.1           232934         25      9317.4     2952     33385  fopen         \n",
      "     0.0           114782         12      9565.2     4483     15753  write         \n",
      "     0.0            73697          8      9212.1     2111     25381  fgetc         \n",
      "     0.0            65475          5     13095.0     8094     18547  open          \n",
      "     0.0            57389         39      1471.5     1024      6902  fcntl         \n",
      "     0.0            47460          8      5932.5     1552     10864  munmap        \n",
      "     0.0            44001         18      2444.5     1467      5871  fclose        \n",
      "     0.0            31786         14      2270.4     1132      5407  read          \n",
      "     0.0            30074          2     15037.0    14931     15143  socket        \n",
      "     0.0            14423          1     14423.0    14423     14423  connect       \n",
      "     0.0            10199          2      5099.5     4546      5653  fread         \n",
      "     0.0             9179          1      9179.0     9179      9179  pipe2         \n",
      "     0.0             5050          1      5050.0     5050      5050  bind          \n",
      "     0.0             3151          1      3151.0     3151      3151  listen        \n",
      "\n",
      "Report file moved to \"/dli/task/print-numbers-in-streams-report.qdrep\"\n",
      "Report file moved to \"/dli/task/print-numbers-in-streams-report.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true -o print-numbers-in-streams-report print-numbers-in-streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![streams print](images/streams-print.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Use Streams for Concurrent Data Initialization Kernels\n",
    "\n",
    "The vector addition application you have been working with, [01-prefetch-check-solution.cu](../edit/04-prefetch-check/solutions/01-prefetch-check-solution.cu), currently launches an initialization kernel 3 times - once each for each of the 3 vectors needing initialization for the `vectorAdd` kernel. Refactor it to launch each of the 3 initialization kernel launches in their own non-default stream. You should still see the success message print when compiling and running with the code execution cell below. Refer to [the solution](../edit/06-stream-init/solutions/01-stream-init-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o init-in-streams 04-prefetch-check/solutions/01-prefetch-check-solution.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open a report in Nsight Systems to confirm that your 3 initialization kernel launches are running in their own non-default streams, with some degree of concurrent overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-17d6-d936-11eb-f796.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-17d6-d936-11eb-f796.qdrep\"\n",
      "Exporting 1204 events: [==================================================100%]12%                                                ]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-17d6-d936-11eb-f796.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls    Average    Minimum   Maximum           Name         \n",
      " -------  ---------------  ---------  -----------  -------  ---------  ---------------------\n",
      "    81.8        315497434          3  105165811.3    32930  315372369  cudaMallocManaged    \n",
      "    14.5         55947591          4   13986897.8   909007   53096965  cudaMemPrefetchAsync \n",
      "     2.8         10799905          3    3599968.3   813785    9047715  cudaFree             \n",
      "     0.9          3500871          1    3500871.0  3500871    3500871  cudaDeviceSynchronize\n",
      "     0.0            76223          4      19055.8     9197      43854  cudaLaunchKernel     \n",
      "     0.0            44957          3      14985.7     3868      36698  cudaStreamCreate     \n",
      "     0.0            27459          3       9153.0     4288      13360  cudaStreamDestroy    \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances   Average   Minimum  Maximum                     Name                    \n",
      " -------  ---------------  ---------  ---------  -------  -------  -------------------------------------------\n",
      "    54.3          2033500          3   677833.3   642325   706099  initWith(float, float*, int)               \n",
      "    45.7          1709602          1  1709602.0  1709602  1709602  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average   Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  --------  -------  -------  ---------------------------------\n",
      "   100.0         19927778          64  311371.5   310875   317499  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average   Minimum   Maximum               Operation            \n",
      " ----------  ----------  --------  --------  --------  ---------------------------------\n",
      " 131072.000          64  2048.000  2048.000  2048.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum        Name     \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------\n",
      "    68.5        562000770         31  18129057.1    29137  100136708  poll          \n",
      "    21.2        173990207        693    251068.1     1039   53012328  ioctl         \n",
      "     8.2         67301934         26   2588535.9    21505   20931155  sem_timedwait \n",
      "     1.8         14484539         94    154090.8     1457    8982582  mmap          \n",
      "     0.3          2225375         82     27138.7     8036      44401  open64        \n",
      "     0.0           202141          4     50535.3    43668      59950  pthread_create\n",
      "     0.0           168433          3     56144.3    53400      60066  fgets         \n",
      "     0.0           168194         25      6727.8     1564      28168  fopen         \n",
      "     0.0            96545         11      8776.8     5552      15085  write         \n",
      "     0.0            59096         11      5372.4     2045      10309  munmap        \n",
      "     0.0            50016         37      1351.8     1000       6790  fcntl         \n",
      "     0.0            37490          5      7498.0     4576       9192  open          \n",
      "     0.0            32092         18      1782.9     1121       6701  fclose        \n",
      "     0.0            29197         13      2245.9     1343       4024  read          \n",
      "     0.0            28772          6      4795.3     1059      13920  fgetc         \n",
      "     0.0            19794          2      9897.0     8885      10909  socket        \n",
      "     0.0            15202          1     15202.0    15202      15202  sem_wait      \n",
      "     0.0            13448          4      3362.0     2232       5066  fread         \n",
      "     0.0             9325          1      9325.0     9325       9325  connect       \n",
      "     0.0             8418          1      8418.0     8418       8418  pipe2         \n",
      "     0.0             3046          1      3046.0     3046       3046  bind          \n",
      "     0.0             1901          1      1901.0     1901       1901  listen        \n",
      "\n",
      "Report file moved to \"/dli/task/init-in-streams-report.qdrep\"\n",
      "Report file moved to \"/dli/task/init-in-streams-report.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true --force-overwrite true -o init-in-streams-report ./init-in-streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "At this point in the lab you are able to:\n",
    "\n",
    "- Use the **Nsight Systems** to visually profile the timeline of GPU-accelerated CUDA applications.\n",
    "- Use Nsight Systems to identify, and exploit, optimization opportunities in GPU-accelerated CUDA applications.\n",
    "- Utilize CUDA streams for concurrent kernel execution in accelerated applications.\n",
    "\n",
    "At this point in time you have a wealth of fundamental tools and techniques for accelerating CPU-only applications, and for then optimizing those accelerated applications. In the final exercise, you will have a chance to apply everything that you've learned to accelerate an [n-body](https://en.wikipedia.org/wiki/N-body_problem) simulator, which predicts the individual motions of a group of objects interacting with each other gravitationally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Final Exercise: Accelerate and Optimize an N-Body Simulator\n",
    "\n",
    "An [n-body](https://en.wikipedia.org/wiki/N-body_problem) simulator predicts the individual motions of a group of objects interacting with each other gravitationally. [01-nbody.cu](../edit/09-nbody/01-nbody.cu) contains a simple, though working, n-body simulator for bodies moving through 3 dimensional space.\n",
    "\n",
    "In its current CPU-only form, this application takes about 5 seconds to run on 4096 particles, and **20 minutes** to run on 65536 particles. Your task is to GPU accelerate the program, retaining the correctness of the simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considerations to Guide Your Work\n",
    "\n",
    "Here are some things to consider before beginning your work:\n",
    "\n",
    "- Especially for your first refactors, the logic of the application, the `bodyForce` function in particular, can and should remain largely unchanged: focus on accelerating it as easily as possible.\n",
    "- The code base contains a for-loop inside `main` for integrating the interbody forces calculated by `bodyForce` into the positions of the bodies in the system. This integration both needs to occur after `bodyForce` runs, and, needs to complete before the next call to `bodyForce`. Keep this in mind when choosing how and where to parallelize.\n",
    "- Use a **profile driven** and iterative approach.\n",
    "- You are not required to add error handling to your code, but you might find it helpful, as you are responsible for your code working correctly.\n",
    "\n",
    "**Have Fun!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this cell to compile the nbody simulator. Although it is initially a CPU-only application, is does accurately simulate the positions of the particles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!nvcc -std=c++11 -o nbody 09-nbody/01-nbody.cu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is highly recommended you use the profiler to assist your work. Execute the following cell to generate a report file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "20.356 Billion Interactions / second\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-0f29-4959-e26b-9e8b.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-0f29-4959-e26b-9e8b.qdrep\"\n",
      "Exporting 1171 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-0f29-4959-e26b-9e8b.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls    Average     Minimum    Maximum           Name         \n",
      " -------  ---------------  ---------  -----------  ---------  ---------  ---------------------\n",
      "    97.1        291114409          1  291114409.0  291114409  291114409  cudaMallocManaged    \n",
      "     2.7          8075087         20     403754.4       6153     999006  cudaDeviceSynchronize\n",
      "     0.1           275619          1     275619.0     275619     275619  cudaMemPrefetchAsync \n",
      "     0.0           146477         20       7323.9       3933      36918  cudaLaunchKernel     \n",
      "     0.0           130038          1     130038.0     130038     130038  cudaFree             \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances  Average   Minimum  Maximum                  Name                 \n",
      " -------  ---------------  ---------  --------  -------  -------  -------------------------------------\n",
      "    99.4          7974463         10  797446.3   765974   997396  bodyForce(Body*, float, int)         \n",
      "     0.6            48704         10    4870.4     4768     5440  integratePositions(Body*, float, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
      "    62.4            41308           8   5163.5     1471    10048  [CUDA Unified Memory memcpy DtoH]\n",
      "    37.6            24895           2  12447.5     6912    17983  [CUDA Unified Memory memcpy HtoD]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "  Total   Operations  Average  Minimum  Maximum              Operation            \n",
      " -------  ----------  -------  -------  -------  ---------------------------------\n",
      " 224.000           8   28.000    4.000   60.000  [CUDA Unified Memory memcpy DtoH]\n",
      " 128.000           2   64.000   32.000   96.000  [CUDA Unified Memory memcpy HtoD]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum        Name     \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------\n",
      "    64.4        341711584         18  18983976.9    23394  100122862  poll          \n",
      "    26.8        141900119        679    208984.0     1025   33943432  ioctl         \n",
      "     7.1         37561867         14   2682990.5    14965   20596486  sem_timedwait \n",
      "     0.9          4848808         88     55100.1     3012    2124574  mmap          \n",
      "     0.4          2338651         82     28520.1    14257      64775  open64        \n",
      "     0.1           372028          3    124009.3   120248     130214  fgets         \n",
      "     0.1           370600          4     92650.0    81276     103606  pthread_create\n",
      "     0.0           232364         25      9294.6     2720      33728  fopen         \n",
      "     0.0           195990         14     13999.3     1133     167723  read          \n",
      "     0.0           195875          1    195875.0   195875     195875  writev        \n",
      "     0.0           104914         11      9537.6     3840      19150  write         \n",
      "     0.0           102122         20      5106.1     1487      48636  fclose        \n",
      "     0.0            98997          1     98997.0    98997      98997  sem_wait      \n",
      "     0.0            83940          2     41970.0    17166      66774  fopen64       \n",
      "     0.0            73896          5     14779.2     7671      19218  open          \n",
      "     0.0            65777         44      1494.9     1004       6407  fcntl         \n",
      "     0.0            50859          6      8476.5     1863      25468  fgetc         \n",
      "     0.0            48556          7      6936.6     3121      14422  munmap        \n",
      "     0.0            33034          2     16517.0    14771      18263  socket        \n",
      "     0.0            18465          1     18465.0    18465      18465  connect       \n",
      "     0.0            16304          4      4076.0     3750       4614  fread         \n",
      "     0.0             9579          1      9579.0     9579       9579  pipe2         \n",
      "     0.0             5491          1      5491.0     5491       5491  bind          \n",
      "     0.0             3103          1      3103.0     3103       3103  listen        \n",
      "\n",
      "Report file moved to \"/dli/task/nbody-report.qdrep\"\n",
      "Report file moved to \"/dli/task/nbody-report.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true --force-overwrite=true -o nbody-report ./nbody"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import a function that will run your `nbody` simulator against a various number of particles, checking for performance and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from assessment import run_assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following cell to run and assess `nbody`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running nbody simulator with 4096 bodies\n",
      "----------------------------------------\n",
      "\n",
      "Application should run faster than 0.9s\n",
      "Your application ran in: 0.2697s\n",
      "Your application reports  20.707 Billion Interactions / second\n",
      "\n",
      "Your results are correct\n",
      "\n",
      "Running nbody simulator with 65536 bodies\n",
      "----------------------------------------\n",
      "\n",
      "Application should run faster than 1.3s\n",
      "Your application ran in: 0.5413s\n",
      "Your application reports  119.453 Billion Interactions / second\n",
      "\n",
      "Your results are correct\n",
      "\n",
      "Congratulations! You passed the assessment!\n",
      "See instructions below to generate a certificate, and see if you can accelerate the simulator even more!\n"
     ]
    }
   ],
   "source": [
    "run_assessment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive GPU Solution\n",
    "    \n",
    "```\n",
    "Running nbody simulator with 4096 bodies\n",
    "----------------------------------------\n",
    "\n",
    "Application should run faster than 0.9s\n",
    "Your application ran in: 0.2171s\n",
    "Your application reports  15.856 Billion Interactions / second\n",
    "\n",
    "Your results are correct\n",
    "\n",
    "Running nbody simulator with 65536 bodies\n",
    "----------------------------------------\n",
    "\n",
    "Application should run faster than 1.3s\n",
    "Your application ran in: 0.5902s\n",
    "Your application reports  119.475 Billion Interactions / second\n",
    "\n",
    "Your results are correct\n",
    "```\n",
    "\n",
    "```\n",
    "CUDA Kernel Statistics:\n",
    "\n",
    " Time(%)  Total Time (ns)  Instances   Average   Minimum  Maximum              Name            \n",
    " -------  ---------------  ---------  ---------  -------  -------  ----------------------------\n",
    "   100.0         10160351         10  1016035.1   959345  1421930  bodyForce(Body*, float, int)\n",
    "\n",
    "CUDA Memory Operation Statistics (by time):\n",
    "\n",
    " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
    " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
    "    52.7           232525          80   2906.6     1407     9952  [CUDA Unified Memory memcpy DtoH]\n",
    "    47.3           208759          15  13917.3     6784    17504  [CUDA Unified Memory memcpy HtoD]\n",
    "\n",
    "CUDA Memory Operation Statistics (by size in KiB):\n",
    "\n",
    "  Total    Operations  Average  Minimum  Maximum              Operation            \n",
    " --------  ----------  -------  -------  -------  ---------------------------------\n",
    " 1120.000          15   74.667   32.000   96.000  [CUDA Unified Memory memcpy HtoD]\n",
    " 1088.000          80   13.600    4.000   60.000  [CUDA Unified Memory memcpy DtoH]\n",
    "```\n",
    "\n",
    "### Position Integration Kernel\n",
    "\n",
    "This has two benefits:\n",
    "\n",
    "- No more passing the buffer back and forth between host and device every time we alternative between `bodyForce` and position integration\n",
    "- Parallelize position integration\n",
    "\n",
    "```\n",
    "Running nbody simulator with 4096 bodies\n",
    "----------------------------------------\n",
    "\n",
    "Application should run faster than 0.9s\n",
    "Your application ran in: 0.2354s\n",
    "Your application reports  20.448 Billion Interactions / second\n",
    "\n",
    "Your results are correct\n",
    "\n",
    "Running nbody simulator with 65536 bodies\n",
    "----------------------------------------\n",
    "\n",
    "Application should run faster than 1.3s\n",
    "Your application ran in: 0.5131s\n",
    "Your application reports  128.577 Billion Interactions / second\n",
    "\n",
    "Your results are correct\n",
    "```\n",
    "\n",
    "```\n",
    "CUDA Kernel Statistics:\n",
    "\n",
    " Time(%)  Total Time (ns)  Instances  Average   Minimum  Maximum                  Name                 \n",
    " -------  ---------------  ---------  --------  -------  -------  -------------------------------------\n",
    "    99.4          8348692         10  834869.2   762772  1372584  bodyForce(Body*, float, int)         \n",
    "     0.6            49183         10    4918.3     4768     5376  integratePositions(Body*, float, int)\n",
    "\n",
    "\n",
    "\n",
    "CUDA Memory Operation Statistics (by time):\n",
    "\n",
    " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
    " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
    "    51.4            24127           2  12063.5     6848    17279  [CUDA Unified Memory memcpy HtoD]\n",
    "    48.6            22781           4   5695.3     1407     9952  [CUDA Unified Memory memcpy DtoH]\n",
    "\n",
    "\n",
    "\n",
    "CUDA Memory Operation Statistics (by size in KiB):\n",
    "\n",
    "  Total   Operations  Average  Minimum  Maximum              Operation            \n",
    " -------  ----------  -------  -------  -------  ---------------------------------\n",
    " 128.000           4   32.000    4.000   60.000  [CUDA Unified Memory memcpy DtoH]\n",
    " 128.000           2   64.000   32.000   96.000  [CUDA Unified Memory memcpy HtoD]\n",
    "\n",
    "```\n",
    "\n",
    "Quantitative benefits:\n",
    "\n",
    "- total time from 10160351ns to 8348692ns + 49183 = 8397875ns (10x speedup)\n",
    "- memory operation time from 232525ns + 208759ns = 441284ns to 24127ns + 22781ns = 46908ns (10x speedup)\n",
    "\n",
    "Interestingly, the overall application time hasn't changed that much; while nsys shows a massive speedup in terms of kernel time, the assessement script only sees an incremental improvement for the sim with many bodies, and a slight slowdown for the smaller simulator.\n",
    "\n",
    "### Optimal number of threads and blocks\n",
    "\n",
    "Messing around with the number of threads and blocks, such as making the number of blocks a multiple of the number of streaming multiprocessors, I couldn't get better performance than my fairly standard configuration of:\n",
    "\n",
    "```\n",
    "int threadsPerBlock = 256;\n",
    "int numberOfBlocks = (nBodies + threadsPerBlock - 1) / threadsPerBlock;\n",
    "```\n",
    "\n",
    "For example, with:\n",
    "\n",
    "```\n",
    "int threadsPerBlock = 64;\n",
    "int numberOfBlocks = 32 * multiProcessorCount;\n",
    "```\n",
    "\n",
    "```\n",
    "Running nbody simulator with 4096 bodies\n",
    "----------------------------------------\n",
    "\n",
    "Application should run faster than 0.9s\n",
    "Your application ran in: 0.2544s\n",
    "Your application reports  21.226 Billion Interactions / second\n",
    "\n",
    "Your results are correct\n",
    "\n",
    "Running nbody simulator with 65536 bodies\n",
    "----------------------------------------\n",
    "\n",
    "Application should run faster than 1.3s\n",
    "Your application ran in: 0.5426s\n",
    "Your application reports  120.916 Billion Interactions / second\n",
    "```\n",
    "\n",
    "```\n",
    "CUDA Kernel Statistics:\n",
    "\n",
    " Time(%)  Total Time (ns)  Instances  Average   Minimum  Maximum                  Name                 \n",
    " -------  ---------------  ---------  --------  -------  -------  -------------------------------------\n",
    "    99.2          7849580         10  784958.0   737589  1205102  bodyForce(Body*, float, int)         \n",
    "     0.8            63231         10    6323.1     6112     6752  integratePositions(Body*, float, int)\n",
    "\n",
    "\n",
    "CUDA Memory Operation Statistics (by time):\n",
    "\n",
    " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
    " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
    "    54.6            27357           4   6839.3     2143    13439  [CUDA Unified Memory memcpy HtoD]\n",
    "    45.4            22782           4   5695.5     1439     9920  [CUDA Unified Memory memcpy DtoH]\n",
    "\n",
    "\n",
    "CUDA Memory Operation Statistics (by size in KiB):\n",
    "\n",
    "  Total   Operations  Average  Minimum  Maximum              Operation            \n",
    " -------  ----------  -------  -------  -------  ---------------------------------\n",
    " 128.000           4   32.000    4.000   60.000  [CUDA Unified Memory memcpy DtoH]\n",
    " 128.000           4   32.000    4.000   72.000  [CUDA Unified Memory memcpy HtoD]\n",
    "```\n",
    "\n",
    "We had a slightly faster `bodyForce` kernel, but slower `integratePositions` one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a Certificate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you passed the assessment, please return to the course page (shown below) and click the \"ASSESS TASK\" button, which will generate your certificate for the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![run_assessment](./images/run_assessment.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Content\n",
    "\n",
    "The following sections, for those of you with time and interest, introduce more intermediate techniques involving some manual device memory management, and using non-default streams to overlap kernel execution and memory copies.\n",
    "\n",
    "After learning about each of the techniques below, try to further optimize your nbody simulation using these techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Manual Device Memory Allocation and Copying\n",
    "\n",
    "While `cudaMallocManaged` and `cudaMemPrefetchAsync` are performant, and greatly simplify memory migration, sometimes it can be worth it to use more manual methods for memory allocation. This is particularly true when it is known that data will only be accessed on the device or host, and the cost of migrating data can be reclaimed in exchange for the fact that no automatic on-demand migration is needed.\n",
    "\n",
    "Additionally, using manual device memory management can allow for the use of non-default streams for overlapping data transfers with computational work. In this section you will learn some basic manual device memory allocation and copy techniques, before extending these techniques to overlap data copies with computational work. \n",
    "\n",
    "Here are some CUDA commands for manual device memory management:\n",
    "\n",
    "- `cudaMalloc` will allocate memory directly to the active GPU. This prevents all GPU page faults. In exchange, the pointer it returns is not available for access by host code.\n",
    "- `cudaMallocHost` will allocate memory directly to the CPU. It also \"pins\" the memory, or page locks it, which will allow for asynchronous copying of the memory to and from a GPU. Too much pinned memory can interfere with CPU performance, so use it only with intention. Pinned memory should be freed with `cudaFreeHost`.\n",
    "- `cudaMemcpy` can copy (not transfer) memory, either from host to device or from device to host."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Device Memory Management Example\n",
    "\n",
    "Here is a snippet of code that demonstrates the use of the above CUDA API calls.\n",
    "\n",
    "```cpp\n",
    "int *host_a, *device_a;        // Define host-specific and device-specific arrays.\n",
    "cudaMalloc(&device_a, size);   // `device_a` is immediately available on the GPU.\n",
    "cudaMallocHost(&host_a, size); // `host_a` is immediately available on CPU, and is page-locked, or pinned.\n",
    "\n",
    "initializeOnHost(host_a, N);   // No CPU page faulting since memory is already allocated on the host.\n",
    "\n",
    "// `cudaMemcpy` takes the destination, source, size, and a CUDA-provided variable for the direction of the copy.\n",
    "cudaMemcpy(device_a, host_a, size, cudaMemcpyHostToDevice);\n",
    "\n",
    "kernel<<<blocks, threads, 0, someStream>>>(device_a, N);\n",
    "\n",
    "// `cudaMemcpy` can also copy data from device to host.\n",
    "cudaMemcpy(host_a, device_a, size, cudaMemcpyDeviceToHost);\n",
    "\n",
    "verifyOnHost(host_a, N);\n",
    "\n",
    "cudaFree(device_a);\n",
    "cudaFreeHost(host_a);          // Free pinned memory like this.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Manually Allocate Host and Device Memory\n",
    "\n",
    "The most recent iteration of the vector addition application, [01-stream-init-solution](../edit/06-stream-init/solutions/01-stream-init-solution.cu), is using `cudaMallocManaged` to allocate managed memory first used on the device by the initialization kernels, then on the device by the vector add kernel, and then by the host, where the memory is automatically transferred, for verification. This is a sensible approach, but it is worth experimenting with some manual device memory allocation and copying to observe its impact on the application's performance.\n",
    "\n",
    "Refactor the [01-stream-init-solution](../edit/06-stream-init/solutions/01-stream-init-solution.cu) application to **not** use `cudaMallocManaged`. In order to do this you will need to do the following:\n",
    "\n",
    "- Replace calls to `cudaMallocManaged` with `cudaMalloc`.\n",
    "- Create an additional vector that will be used for verification on the host. This is required since the memory allocated with `cudaMalloc` is not available to the host. Allocate this host vector with `cudaMallocHost`.\n",
    "- After the `addVectorsInto` kernel completes, use `cudaMemcpy` to copy the vector with the addition results, into the host vector you created with `cudaMallocHost`.\n",
    "- Use `cudaFreeHost` to free the memory allocated with `cudaMallocHost`.\n",
    "\n",
    "Refer to [the solution](../edit/07-manual-malloc/solutions/01-manual-malloc-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o vector-add-manual-alloc 06-stream-init/solutions/01-stream-init-solution.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing the refactor, open a report in Nsight Systems, and use the timeline to do the following:\n",
    "\n",
    "- Notice that there is no longer a *Unified Memory* section of the timeline.\n",
    "- Comparing this timeline to that of the previous refactor, compare the run times of `cudaMalloc` in the current application vs. `cudaMallocManaged` in the previous.\n",
    "- Notice how in the current application, work on the initialization kernels does not start until a later time than it did in the previous iteration. Examination of the timeline will show the difference is the time taken by `cudaMallocHost`. This clearly points out the difference between memory transfers, and memory copies. When copying memory, as you are doing presently, the data will exist in 2 different places in the system. In the current case, the allocation of the 4th host-only vector incurs a small cost in performance, compared to only allocating 3 vectors in the previous iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-f5e1-02ce-5c5a-a117.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-f5e1-02ce-5c5a-a117.qdrep\"\n",
      "Exporting 1071 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-f5e1-02ce-5c5a-a117.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum    Maximum           Name         \n",
      " -------  ---------------  ---------  ----------  --------  ---------  ---------------------\n",
      "    67.7        238912401          3  79637467.0    210592  238473403  cudaMalloc           \n",
      "    19.3         68287491          1  68287491.0  68287491   68287491  cudaHostAlloc        \n",
      "     5.6         19865605          1  19865605.0  19865605   19865605  cudaMemcpy           \n",
      "     5.6         19708785          1  19708785.0  19708785   19708785  cudaFreeHost         \n",
      "     1.0          3544402          1   3544402.0   3544402    3544402  cudaDeviceSynchronize\n",
      "     0.7          2588276          3    862758.7    375018    1170812  cudaFree             \n",
      "     0.0            65652          3     21884.0      3767      56898  cudaStreamDestroy    \n",
      "     0.0            64841          4     16210.3      5936      40730  cudaLaunchKernel     \n",
      "     0.0            39876          3     13292.0      2217      34659  cudaStreamCreate     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances   Average   Minimum  Maximum                     Name                    \n",
      " -------  ---------------  ---------  ---------  -------  -------  -------------------------------------------\n",
      "    54.3          2024386          3   674795.3   633463   699957  initWith(float, float*, int)               \n",
      "    45.7          1705543          1  1705543.0  1705543  1705543  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations   Average    Minimum   Maximum       Operation     \n",
      " -------  ---------------  ----------  ----------  --------  --------  ------------------\n",
      "   100.0         19819868           1  19819868.0  19819868  19819868  [CUDA memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations   Average     Minimum     Maximum        Operation     \n",
      " ----------  ----------  ----------  ----------  ----------  ------------------\n",
      " 131072.000           1  131072.000  131072.000  131072.000  [CUDA memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum        Name     \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------\n",
      "    66.4        400450338         15  26696689.2    22718  100138180  poll          \n",
      "    30.1        181321383        687    263932.1     1000   66840487  ioctl         \n",
      "     3.0         17887126         91    196561.8     1280   14803892  mmap          \n",
      "     0.3          1926348         82     23492.0    10417      41353  open64        \n",
      "     0.0           258335         10     25833.5    14894      52715  sem_timedwait \n",
      "     0.0           194708          3     64902.7    53551      81403  fgets         \n",
      "     0.0           189813          4     47453.3    41061      53629  pthread_create\n",
      "     0.0           155132         25      6205.3     1978      24289  fopen         \n",
      "     0.0            83042         11      7549.3     4219      14195  write         \n",
      "     0.0            56250         13      4326.9     1657      11760  munmap        \n",
      "     0.0            37613          5      7522.6     6212       9282  open          \n",
      "     0.0            29924         18      1662.4     1094       3882  fclose        \n",
      "     0.0            24009          5      4801.8     1026      11696  fgetc         \n",
      "     0.0            22227         13      1709.8     1085       2585  read          \n",
      "     0.0            20149          2     10074.5     8880      11269  socket        \n",
      "     0.0            13648          8      1706.0     1021       5999  fcntl         \n",
      "     0.0            13464          1     13464.0    13464      13464  sem_wait      \n",
      "     0.0            12801          4      3200.3     2020       4257  fread         \n",
      "     0.0            10225          1     10225.0    10225      10225  connect       \n",
      "     0.0             8735          1      8735.0     8735       8735  pipe2         \n",
      "     0.0             2603          1      2603.0     2603       2603  bind          \n",
      "     0.0             1739          1      1739.0     1739       1739  listen        \n",
      "\n",
      "Report file moved to \"/dli/task/vector-add-manual-alloc-report.qdrep\"\n",
      "Report file moved to \"/dli/task/vector-add-manual-alloc-report.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true -o vector-add-manual-alloc-report ./vector-add-manual-alloc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Using Streams to Overlap Data Transfers and Code Execution\n",
    "\n",
    "The following slides present upcoming material visually, at a high level. Click through the slides before moving on to more detailed coverage of their topics in following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div align=\"center\"><iframe src=\"https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-01-V1/embedded/task3/NVVP-Streams-3.pptx\" width=\"800px\" height=\"500px\" frameborder=\"0\"></iframe></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "\n",
    "<div align=\"center\"><iframe src=\"https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-01-V1/embedded/task3/NVVP-Streams-3.pptx\" width=\"800px\" height=\"500px\" frameborder=\"0\"></iframe></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to `cudaMemcpy` is `cudaMemcpyAsync` which can asynchronously copy memory either from host to device or from device to host as long as the host memory is pinned, which can be done by allocating it with `cudaMallocHost`.\n",
    "\n",
    "Similar to kernel execution, `cudaMemcpyAsync` is only asynchronous by default with respect to the host. It executes, by default, in the default stream and therefore is a blocking operation with regard to other CUDA operations occurring on the GPU. The `cudaMemcpyAsync` function, however, takes as an optional 5th argument, a non-default stream. By passing it a non-default stream, the memory transfer can be concurrent to other CUDA operations occurring in other non-default streams.\n",
    "\n",
    "A common and useful pattern is to use a combination of pinned host memory, asynchronous memory copies in non-default streams, and kernel executions in non-default streams, to overlap memory transfers with kernel execution.\n",
    "\n",
    "In the following example, rather than wait for the entire memory copy to complete before beginning work on the kernel, segments of the required data are copied and worked on, with each copy/work segment running in its own non-default stream. Using this technique, work on parts of the data can begin while memory transfers for later segments occur concurrently. Extra care must be taken when using this technique to calculate segment-specific values for the number of operations, and the offset location inside arrays, as shown here:\n",
    "\n",
    "```cpp\n",
    "int N = 2<<24;\n",
    "int size = N * sizeof(int);\n",
    "\n",
    "int *host_array;\n",
    "int *device_array;\n",
    "\n",
    "cudaMallocHost(&host_array, size);               // Pinned host memory allocation.\n",
    "cudaMalloc(&device_array, size);                 // Allocation directly on the active GPU device.\n",
    "\n",
    "initializeData(host_array, N);                   // Assume this application needs to initialize on the host.\n",
    "\n",
    "const int numberOfSegments = 4;                  // This example demonstrates slicing the work into 4 segments.\n",
    "int segmentN = N / numberOfSegments;             // A value for a segment's worth of `N` is needed.\n",
    "size_t segmentSize = size / numberOfSegments;    // A value for a segment's worth of `size` is needed.\n",
    "\n",
    "// For each of the 4 segments...\n",
    "for (int i = 0; i < numberOfSegments; ++i)\n",
    "{\n",
    "  // Calculate the index where this particular segment should operate within the larger arrays.\n",
    "  segmentOffset = i * segmentN;\n",
    "\n",
    "  // Create a stream for this segment's worth of copy and work.\n",
    "  cudaStream_t stream;\n",
    "  cudaStreamCreate(&stream);\n",
    "  \n",
    "  // Asynchronously copy segment's worth of pinned host memory to device over non-default stream.\n",
    "  cudaMemcpyAsync(&device_array[segmentOffset],  // Take care to access correct location in array.\n",
    "                  &host_array[segmentOffset],    // Take care to access correct location in array.\n",
    "                  segmentSize,                   // Only copy a segment's worth of memory.\n",
    "                  cudaMemcpyHostToDevice,\n",
    "                  stream);                       // Provide optional argument for non-default stream.\n",
    "                  \n",
    "  // Execute segment's worth of work over same non-default stream as memory copy.\n",
    "  kernel<<<number_of_blocks, threads_per_block, 0, stream>>>(&device_array[segmentOffset], segmentN);\n",
    "  \n",
    "  // `cudaStreamDestroy` will return immediately (is non-blocking), but will not actually destroy stream until\n",
    "  // all stream operations are complete.\n",
    "  cudaStreamDestroy(stream);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Overlap Kernel Execution and Memory Copy Back to Host\n",
    "\n",
    "The most recent iteration of the vector addition application, [01-manual-malloc-solution.cu](../edit/07-manual-malloc/solutions/01-manual-malloc-solution.cu), is currently performing all of its vector addition work on the GPU before copying the memory back to the host for verification.\n",
    "\n",
    "Refactor [01-manual-malloc-solution.cu](../edit/07-manual-malloc/solutions/01-manual-malloc-solution.cu) to perform the vector addition in 4 segments, in non-default streams, so that asynchronous memory copies can begin before waiting for all vector addition work to complete. Refer to [the solution](../edit/08-overlap-xfer/solutions/01-overlap-xfer-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "WARNING: The command line includes a target application therefore the CPU context-switch scope has been set to process-tree.\n",
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-c916-dca0-eefc-f073.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-c916-dca0-eefc-f073.qdrep\"\n",
      "Exporting 1101 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-c916-dca0-eefc-f073.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum    Maximum           Name         \n",
      " -------  ---------------  ---------  ----------  --------  ---------  ---------------------\n",
      "    68.3        211233216          3  70411072.0    217750  210782344  cudaMalloc           \n",
      "    17.2         53259897          1  53259897.0  53259897   53259897  cudaHostAlloc        \n",
      "     7.1         21999120          2  10999560.0   1812698   20186422  cudaDeviceSynchronize\n",
      "     6.4         19734465          1  19734465.0  19734465   19734465  cudaFreeHost         \n",
      "     0.8          2598719          3    866239.7    391920    1175960  cudaFree             \n",
      "     0.0           105146          7     15020.9      6177      39019  cudaLaunchKernel     \n",
      "     0.0            78168          7     11166.9      3163      54409  cudaStreamDestroy    \n",
      "     0.0            51128          7      7304.0      2263      32348  cudaStreamCreate     \n",
      "     0.0            32380          4      8095.0      4513      16951  cudaMemcpyAsync      \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances  Average   Minimum  Maximum                     Name                    \n",
      " -------  ---------------  ---------  --------  -------  -------  -------------------------------------------\n",
      "    53.2          2018528          3  672842.7   629494   702037  initWith(float, float*, int)               \n",
      "    46.8          1773348          4  443337.0   424057   461273  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations   Average   Minimum  Maximum      Operation     \n",
      " -------  ---------------  ----------  ---------  -------  -------  ------------------\n",
      "   100.0         19815177           4  4953794.3  4951634  4958770  [CUDA memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations   Average    Minimum    Maximum       Operation     \n",
      " ----------  ----------  ---------  ---------  ---------  ------------------\n",
      " 131072.000           4  32768.000  32768.000  32768.000  [CUDA memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum        Name     \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------\n",
      "    64.0        300643348         14  21474524.9    53779  100134078  poll          \n",
      "    31.5        147934277        690    214397.5     1014   51815054  ioctl         \n",
      "     3.8         18070851         91    198580.8     1444   14981051  mmap          \n",
      "     0.3          1643258         82     20039.7    10041      36383  open64        \n",
      "     0.1           588437         10     58843.7    13236     301603  sem_timedwait \n",
      "     0.1           243012          3     81004.0    72638      92103  fgets         \n",
      "     0.0           230735          4     57683.8    49130      65981  pthread_create\n",
      "     0.0           198875         25      7955.0     2172      29593  fopen         \n",
      "     0.0            93555         11      8505.0     4173      17727  write         \n",
      "     0.0            69718         14      4979.9     1949      15173  munmap        \n",
      "     0.0            47984          5      9596.8     7937      11985  open          \n",
      "     0.0            39561         18      2197.8     1367       5121  fclose        \n",
      "     0.0            36732          6      6122.0     1656      16586  fgetc         \n",
      "     0.0            25213         12      2101.1     1169       3486  read          \n",
      "     0.0            22102          2     11051.0     9568      12534  socket        \n",
      "     0.0            17843         13      1372.5     1003       4513  fcntl         \n",
      "     0.0            17108          1     17108.0    17108      17108  sem_wait      \n",
      "     0.0            13405          4      3351.3     1964       4626  fread         \n",
      "     0.0            12870          1     12870.0    12870      12870  connect       \n",
      "     0.0             8513          1      8513.0     8513       8513  pipe2         \n",
      "     0.0             3389          1      3389.0     3389       3389  bind          \n",
      "     0.0             2040          1      2040.0     2040       2040  listen        \n",
      "\n",
      "Report file moved to \"/dli/task/vector-add-manual-alloc-report-overlap.qdrep\"\n",
      "Report file moved to \"/dli/task/vector-add-manual-alloc-report-overlap.sqlite\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o vector-add-manual-alloc 07-manual-malloc/solutions/01-manual-malloc-solution.cu -run\n",
    "!nsys profile --stats=true --force-overwrite true -o vector-add-manual-alloc-report-overlap ./vector-add-manual-alloc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing the refactor, open a report in Nsight Systems, and use the timeline to do the following:\n",
    "\n",
    "- Note when the device to host memory transfers begin, is it before or after all kernel work has completed?\n",
    "    - Memory transfers begin before all kernel work has completed\n",
    "- Notice that the 4 memory copy segments themselves do not overlap. Even in separate non-default streams, only one memory transfer in a given direction (DtoH here) at a time can occur simultaneously. The performance gains here are in the ability to start the transfers earlier than otherwise, and it is not hard to imagine in an application where a less trivial amount of work was being done compared to a simple addition operation, that the memory copies would not only start earlier, but also overlap with kernel execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
